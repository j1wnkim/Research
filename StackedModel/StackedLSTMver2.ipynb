{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np \n",
    "import torch \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainCsv =  pd.read_csv(\"/home/jik19004/FilesToRun/PercentileDataAnalysis/newTrainDf.csv\")[[\"Hour\",\"WeekDay or Weekend\",\"Sknt_hourly\",\"Tmpf_Hourly\", \"100\", \"Demand\"]]\n",
    "ValidationCsv = pd.read_csv(\"/home/jik19004/FilesToRun/PercentileDataAnalysis/newValidationDf.csv\")[[\"Hour\",\"WeekDay or Weekend\",\"Sknt_hourly\",\"Tmpf_Hourly\", \"100\",\"Demand\"]]\n",
    "TestingCsv = pd.read_csv(\"/home/jik19004/FilesToRun/PercentileDataAnalysis/newTestingDf.csv\")[[\"Hour\",\"WeekDay or Weekend\",\"Sknt_hourly\",\"Tmpf_Hourly\", \"100\",\"Demand\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainUncertainty = pd.read_csv(\"/home/jik19004/FilesToRun/StackedEnsemblePred/StackedTrainVer2.csv\")[[\"energy_demand_mean\", \"lower_bound\",\"upper_bound\"]]\n",
    "ValUncertainty = pd.read_csv(\"/home/jik19004/FilesToRun/StackedEnsemblePred/StackedValVer2.csv\")[[\"energy_demand_mean\", \"lower_bound\",\"upper_bound\"]]\n",
    "TestUncertainty = pd.read_csv(\"/home/jik19004/FilesToRun/StackedEnsemblePred/StackedTestVer2.csv\")[[\"energy_demand_mean\", \"lower_bound\",\"upper_bound\"]]\n",
    "\n",
    "TrainCsv = pd.concat([TrainCsv, TrainUncertainty], axis=1)\n",
    "ValidationCsv = pd.concat([ValidationCsv, ValUncertainty], axis=1)\n",
    "TestingCsv = pd.concat([TestingCsv, TestUncertainty], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hour</th>\n",
       "      <th>WeekDay or Weekend</th>\n",
       "      <th>Sknt_hourly</th>\n",
       "      <th>Tmpf_Hourly</th>\n",
       "      <th>100</th>\n",
       "      <th>Demand</th>\n",
       "      <th>energy_demand_mean</th>\n",
       "      <th>lower_bound</th>\n",
       "      <th>upper_bound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>44.285000</td>\n",
       "      <td>3640.257568</td>\n",
       "      <td>3858.000</td>\n",
       "      <td>3501.8867</td>\n",
       "      <td>3347.3906</td>\n",
       "      <td>3656.3828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>4.958333</td>\n",
       "      <td>42.912500</td>\n",
       "      <td>3609.803223</td>\n",
       "      <td>3757.000</td>\n",
       "      <td>3435.8843</td>\n",
       "      <td>3279.0970</td>\n",
       "      <td>3592.6716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>4.791667</td>\n",
       "      <td>42.732500</td>\n",
       "      <td>3521.481201</td>\n",
       "      <td>3634.000</td>\n",
       "      <td>3347.0496</td>\n",
       "      <td>3205.0925</td>\n",
       "      <td>3489.0066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>42.350000</td>\n",
       "      <td>3349.622559</td>\n",
       "      <td>3471.000</td>\n",
       "      <td>3203.9760</td>\n",
       "      <td>3077.3188</td>\n",
       "      <td>3330.6333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>42.507500</td>\n",
       "      <td>3082.854248</td>\n",
       "      <td>3238.000</td>\n",
       "      <td>2945.3070</td>\n",
       "      <td>2831.8362</td>\n",
       "      <td>3058.7776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61340</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>6.875000</td>\n",
       "      <td>4387.694336</td>\n",
       "      <td>4682.333</td>\n",
       "      <td>4193.2417</td>\n",
       "      <td>4020.4412</td>\n",
       "      <td>4366.0420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61341</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>5.125000</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>4279.734375</td>\n",
       "      <td>4516.193</td>\n",
       "      <td>4142.3975</td>\n",
       "      <td>3976.5447</td>\n",
       "      <td>4308.2505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61342</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>4215.001465</td>\n",
       "      <td>4349.559</td>\n",
       "      <td>4048.0935</td>\n",
       "      <td>3877.6084</td>\n",
       "      <td>4218.5786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61343</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>5.137500</td>\n",
       "      <td>4041.653076</td>\n",
       "      <td>4173.255</td>\n",
       "      <td>3877.3296</td>\n",
       "      <td>3710.7612</td>\n",
       "      <td>4043.8980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61344</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>6.625000</td>\n",
       "      <td>4.579167</td>\n",
       "      <td>3853.084229</td>\n",
       "      <td>4019.408</td>\n",
       "      <td>3690.0280</td>\n",
       "      <td>3520.7249</td>\n",
       "      <td>3859.3313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61345 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Hour  WeekDay or Weekend  Sknt_hourly  Tmpf_Hourly          100  \\\n",
       "0        18                   0     4.875000    44.285000  3640.257568   \n",
       "1        19                   0     4.958333    42.912500  3609.803223   \n",
       "2        20                   0     4.791667    42.732500  3521.481201   \n",
       "3        21                   0     5.250000    42.350000  3349.622559   \n",
       "4        22                   0     4.875000    42.507500  3082.854248   \n",
       "...     ...                 ...          ...          ...          ...   \n",
       "61340    19                   0     5.625000     6.875000  4387.694336   \n",
       "61341    20                   0     5.125000     5.750000  4279.734375   \n",
       "61342    21                   0     5.625000     5.500000  4215.001465   \n",
       "61343    22                   0     4.250000     5.137500  4041.653076   \n",
       "61344    23                   0     6.625000     4.579167  3853.084229   \n",
       "\n",
       "         Demand  energy_demand_mean  lower_bound  upper_bound  \n",
       "0      3858.000           3501.8867    3347.3906    3656.3828  \n",
       "1      3757.000           3435.8843    3279.0970    3592.6716  \n",
       "2      3634.000           3347.0496    3205.0925    3489.0066  \n",
       "3      3471.000           3203.9760    3077.3188    3330.6333  \n",
       "4      3238.000           2945.3070    2831.8362    3058.7776  \n",
       "...         ...                 ...          ...          ...  \n",
       "61340  4682.333           4193.2417    4020.4412    4366.0420  \n",
       "61341  4516.193           4142.3975    3976.5447    4308.2505  \n",
       "61342  4349.559           4048.0935    3877.6084    4218.5786  \n",
       "61343  4173.255           3877.3296    3710.7612    4043.8980  \n",
       "61344  4019.408           3690.0280    3520.7249    3859.3313  \n",
       "\n",
       "[61345 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(TrainCsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hour</th>\n",
       "      <th>WeekDay or Weekend</th>\n",
       "      <th>Sknt_hourly</th>\n",
       "      <th>Tmpf_Hourly</th>\n",
       "      <th>Demand</th>\n",
       "      <th>energy_demand_mean</th>\n",
       "      <th>lower_bound</th>\n",
       "      <th>upper_bound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>44.285000</td>\n",
       "      <td>3858.000</td>\n",
       "      <td>3501.8867</td>\n",
       "      <td>3347.3906</td>\n",
       "      <td>3656.3828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>4.958333</td>\n",
       "      <td>42.912500</td>\n",
       "      <td>3757.000</td>\n",
       "      <td>3435.8843</td>\n",
       "      <td>3279.0970</td>\n",
       "      <td>3592.6716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>4.791667</td>\n",
       "      <td>42.732500</td>\n",
       "      <td>3634.000</td>\n",
       "      <td>3347.0496</td>\n",
       "      <td>3205.0925</td>\n",
       "      <td>3489.0066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>42.350000</td>\n",
       "      <td>3471.000</td>\n",
       "      <td>3203.9760</td>\n",
       "      <td>3077.3188</td>\n",
       "      <td>3330.6333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4.875000</td>\n",
       "      <td>42.507500</td>\n",
       "      <td>3238.000</td>\n",
       "      <td>2945.3070</td>\n",
       "      <td>2831.8362</td>\n",
       "      <td>3058.7776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61340</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>6.875000</td>\n",
       "      <td>4682.333</td>\n",
       "      <td>4193.2417</td>\n",
       "      <td>4020.4412</td>\n",
       "      <td>4366.0420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61341</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>5.125000</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>4516.193</td>\n",
       "      <td>4142.3975</td>\n",
       "      <td>3976.5447</td>\n",
       "      <td>4308.2505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61342</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>4349.559</td>\n",
       "      <td>4048.0935</td>\n",
       "      <td>3877.6084</td>\n",
       "      <td>4218.5786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61343</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>5.137500</td>\n",
       "      <td>4173.255</td>\n",
       "      <td>3877.3296</td>\n",
       "      <td>3710.7612</td>\n",
       "      <td>4043.8980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61344</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>6.625000</td>\n",
       "      <td>4.579167</td>\n",
       "      <td>4019.408</td>\n",
       "      <td>3690.0280</td>\n",
       "      <td>3520.7249</td>\n",
       "      <td>3859.3313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61345 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Hour  WeekDay or Weekend  Sknt_hourly  Tmpf_Hourly    Demand  \\\n",
       "0        18                   0     4.875000    44.285000  3858.000   \n",
       "1        19                   0     4.958333    42.912500  3757.000   \n",
       "2        20                   0     4.791667    42.732500  3634.000   \n",
       "3        21                   0     5.250000    42.350000  3471.000   \n",
       "4        22                   0     4.875000    42.507500  3238.000   \n",
       "...     ...                 ...          ...          ...       ...   \n",
       "61340    19                   0     5.625000     6.875000  4682.333   \n",
       "61341    20                   0     5.125000     5.750000  4516.193   \n",
       "61342    21                   0     5.625000     5.500000  4349.559   \n",
       "61343    22                   0     4.250000     5.137500  4173.255   \n",
       "61344    23                   0     6.625000     4.579167  4019.408   \n",
       "\n",
       "       energy_demand_mean  lower_bound  upper_bound  \n",
       "0               3501.8867    3347.3906    3656.3828  \n",
       "1               3435.8843    3279.0970    3592.6716  \n",
       "2               3347.0496    3205.0925    3489.0066  \n",
       "3               3203.9760    3077.3188    3330.6333  \n",
       "4               2945.3070    2831.8362    3058.7776  \n",
       "...                   ...          ...          ...  \n",
       "61340           4193.2417    4020.4412    4366.0420  \n",
       "61341           4142.3975    3976.5447    4308.2505  \n",
       "61342           4048.0935    3877.6084    4218.5786  \n",
       "61343           3877.3296    3710.7612    4043.8980  \n",
       "61344           3690.0280    3520.7249    3859.3313  \n",
       "\n",
       "[61345 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.read_csv(\"/home/jik19004/FilesToRun/StackedEnsemblePred/StackedTrainVer2.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "TrainY = TrainCsv[\"Demand\"]\n",
    "ValidationY = ValidationCsv[\"Demand\"]\n",
    "TestingY = TestingCsv[\"Demand\"]\n",
    "\n",
    "TrainX = TrainCsv.drop(\"Demand\", axis = 1)\n",
    "ValidationX = ValidationCsv.drop(\"Demand\", axis = 1)\n",
    "TestingX = TestingCsv.drop(\"Demand\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_sequences(data, outputData, input_n_steps, output_n_steps): \n",
    "    X = [] \n",
    "    Y = [] \n",
    "    length = len(data) \n",
    "    for i in range(0,length, 1): \n",
    "        input_indx = i + input_n_steps \n",
    "        output_indx = input_indx + output_n_steps  \n",
    "        if (output_indx > len(data)): # we need to have equally split sequences.  \n",
    "            break               # The remaining data that cannot fit into a fixed \n",
    "                                # sequence will immediately be cut! \n",
    "        else:\n",
    "            Xsample = data.iloc[i:input_indx, :] # get the previous data \n",
    "            Ysample = outputData[input_indx:output_indx] \n",
    "            X.append(Xsample) \n",
    "            Y.append(Ysample) \n",
    "    X = np.asarray(X).astype('float64') \n",
    "    Y = np.asarray(Y).astype('float64') \n",
    "    return (X, Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleTheData(data):\n",
    "    scaler = StandardScaler()\n",
    "    # split the data first. \n",
    "    data2 = scaler.fit_transform(data)\n",
    "    data = pd.DataFrame(data2, columns = data.columns)\n",
    "    return data\n",
    "\n",
    "def splitDataAndScale(TrainData, ValidationData, TestData, TrainY, ValidationY, TestY,):    \n",
    "    # Janruary 1st 2018. \n",
    "\n",
    "    TrainData = scaleTheData(TrainData.copy())\n",
    "    ValidationData = scaleTheData(ValidationData.copy())\n",
    "    TestingData = scaleTheData(TestData.copy())\n",
    "\n",
    "    TrainingSequences = return_sequences(TrainData, TrainY, 18, 1) \n",
    "\n",
    "    TransformedTrainingData = TrainingSequences[0]\n",
    "    TransformedTrainingOutput = TrainingSequences[1]\n",
    "\n",
    "    ValidationSequences = return_sequences(ValidationData, ValidationY, 18, 1)\n",
    "\n",
    "    TransformedValidationData = ValidationSequences[0]\n",
    "    TransformedValidationOutput = ValidationSequences[1]\n",
    "\n",
    "    TestingSequences = return_sequences(TestingData, TestY, 18, 1)\n",
    "\n",
    "    TransformedTestingData = TestingSequences[0]\n",
    "    TransformedTestingOutput = TestingSequences[1]\n",
    "\n",
    "\n",
    "    return (TransformedTrainingData, TransformedTrainingOutput, TransformedValidationData, TransformedValidationOutput, \n",
    "    TransformedTestingData, TransformedTestingOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = splitDataAndScale(TrainX, ValidationX, TestingX, TrainY, ValidationY, TestingY) # returns a tuple. \n",
    "TrainX = data[0]\n",
    "TrainY = data[1]\n",
    "\n",
    "ValX = data[2]\n",
    "ValY = data[3]\n",
    "\n",
    "TestX = data[4]\n",
    "TestY = data[5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class LSTMModel(torch.nn.Module):\n",
    "    def __init__(self, num_layers, LSTMNeurons, params, output_num =1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.LSTM1 = torch.nn.LSTM(input_size = 8, hidden_size = LSTMNeurons, num_layers = 1, bias = True, batch_first = True)\n",
    "        self.LSTM2 = torch.nn.LSTM(input_size = LSTMNeurons, hidden_size = LSTMNeurons, num_layers = 1, bias = True, batch_first = True, bidirectional = False)\n",
    "        self.batchNorm0 = torch.nn.BatchNorm1d(num_features = 18)\n",
    "\n",
    "\n",
    "        input_size = LSTMNeurons\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            num_units = params[i]\n",
    "            layers.append(torch.nn.Linear(input_size, num_units, bias = True))\n",
    "            layers.append(torch.nn.BatchNorm1d(num_features = 18))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Dropout(0.3))\n",
    "\n",
    "            input_size = num_units\n",
    "        self.intermediateLayers = torch.nn.Sequential(*layers)\n",
    "        self.Linear1 = torch.nn.Linear(in_features = input_size, out_features = LSTMNeurons, bias = True)\n",
    "        self.Activation1 = torch.nn.ReLU()\n",
    "        self.Dropout = torch.nn.Dropout(0.3)\n",
    "\n",
    "\n",
    "        self.Linear2 = torch.nn.Linear(in_features = LSTMNeurons * 18, out_features = output_num, bias = True)\n",
    "        self.Activation2 = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, val):\n",
    "\n",
    "        x = self.LSTM1(val)\n",
    "        x = self.LSTM2(x[0], (x[1][0],x[1][1])) #(x[1][0], x[1][1])\n",
    "        x = self.batchNorm0(x[0])\n",
    "        x = self.Dropout(x)\n",
    "\n",
    "        y = x.clone() # skip connection.\n",
    "\n",
    "        x = self.intermediateLayers(x)\n",
    "        x = self.Linear1(x)\n",
    "        x = self.Activation1(x)\n",
    "        x = self.Dropout(x)\n",
    "        x = x.view(-1, x.size(1) * x.size(2))\n",
    "        y = y.view(-1, y.size(1) * y.size(2))\n",
    "        x = self.Linear2(torch.add(x,y))\n",
    "        x = self.Activation2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch \n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, output):\n",
    "        data = torch.tensor(data).float(); \n",
    "        output = torch.tensor(output).float() \n",
    "        self.data = data \n",
    "        self.output = output; \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx];  \n",
    "        y = self.output[idx]; \n",
    "        \n",
    "        return x, y; \n",
    "\n",
    "TrainDataset = TimeSeriesDataset(np.array(TrainX), np.array(TrainY))\n",
    "ValidationDataset = TimeSeriesDataset(np.array(ValX), np.array(ValY))\n",
    "TestingDataset = TimeSeriesDataset(np.array(TestX), np.array(TestY))\n",
    "\n",
    "TrainingLoader = DataLoader(TrainDataset, batch_size=256, shuffle=True)\n",
    "ValidationLoader = DataLoader(ValidationDataset, batch_size=256, shuffle=True)\n",
    "TestingLoader = DataLoader(TestingDataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            running_loss += loss.item() * target.size(0)\n",
    "    return running_loss / len(val_loader.dataset)\n",
    "\n",
    "def Train_and_Evaluate(train_loader, val_loader, device, params1, params2, numEpochs, early_stop_epochs):\n",
    "    #num_layers, dropout = 0.1, outfeatures1 = 16, outfeatures2 = 16, outfeatures3 = 16, outfeatures4 = 16, dim_feedforward = 2048, output_num = 1\n",
    "    model = LSTMModel(num_layers = params1[0], LSTMNeurons = params1[1], params = params2)\n",
    "    model = model.to(device);\n",
    "    LossFunction = torch.nn.L1Loss();\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "\n",
    "\n",
    "    Optimizer = torch.optim.Adam(params = model.parameters(), weight_decay = 0.01)\n",
    "    for epoch in range(0,numEpochs):\n",
    "        model.train()\n",
    "        Training_Loss = 0;\n",
    "        total_samples = 0;\n",
    "        for input, output in train_loader:\n",
    "            input = input.to(device);\n",
    "            output = torch.squeeze(output, 1);\n",
    "            output = output.to(device);\n",
    "            predictedVal = model(input)\n",
    "            predictedVal = torch.squeeze(predictedVal, 1)\n",
    "            Optimizer.zero_grad();\n",
    "            batchLoss = LossFunction(predictedVal, output);\n",
    "            batchLoss.backward();\n",
    "            Optimizer.step();\n",
    "            Training_Loss += batchLoss * output.size(0) #* output.size(0);\n",
    "            total_samples += output.size(0)\n",
    "        Training_Loss = Training_Loss.item()/total_samples\n",
    "\n",
    "\n",
    "        Validation_Loss = 0;\n",
    "        print(\"passed \", epoch, \"epoch\", \"Training Loss: \", Training_Loss,\" \", end = \"\")\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            total_val_samples = 0;\n",
    "            Validation_Loss = 0;\n",
    "            for val_input, val_output in val_loader:\n",
    "                val_input = val_input.to(device);\n",
    "                val_output = torch.squeeze(val_output,1);\n",
    "                val_output = val_output.to(device);\n",
    "                predictedVal = model(val_input)\n",
    "                predictedVal = torch.squeeze(predictedVal, 1)\n",
    "                Validation_Loss += LossFunction(val_output, predictedVal) * val_output.size(0)\n",
    "                total_val_samples += val_output.size(0)\n",
    "            Validation_Loss = Validation_Loss/total_val_samples\n",
    "            print(\"Validation Loss: \", Validation_Loss)\n",
    "\n",
    "            if Validation_Loss < best_val_loss:\n",
    "                best_val_loss = Validation_Loss\n",
    "                torch.save(model, \"/home/jik19004/FilesToRun/StackedEnsemblePred/StackedLSTMver2\")\n",
    "                early_stop_count = 0;\n",
    "            else:\n",
    "                early_stop_count +=1\n",
    "            if early_stop_count >= early_stop_epochs:\n",
    "                return best_val_loss;\n",
    "    return best_val_loss;\n",
    "\n",
    "def predict(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    act_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for data, _ in data_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            predictions.append(output.cpu().numpy())\n",
    "            act_outputs.append(_.numpy())\n",
    "\n",
    "    return (np.concatenate(predictions), np.concatenate(act_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-21 14:29:56,821] Using an existing study with name 'StackedLSTMver3' instead of creating a new one.\n",
      "[W 2024-06-21 14:30:01,025] Trial 378 failed with parameters: {'num_layers': 3, 'LSTM_neurons': 24, 'num_hiddenZero': 24, 'num_hiddenOne': 36, 'num_hiddenTwo': 48, 'num_hiddenThree': 48, 'num_hiddenFour': 24} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jik19004/miniconda3/envs/CLEAN/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_20205/1829702913.py\", line 13, in objective\n",
      "    return Train_and_Evaluate(TrainingLoader, ValidationLoader, device, params1, params2, 260, 35);\n",
      "  File \"/tmp/ipykernel_20205/2292688360.py\", line 35, in Train_and_Evaluate\n",
      "    batchLoss.backward();\n",
      "  File \"/home/jik19004/miniconda3/envs/CLEAN/lib/python3.10/site-packages/torch/_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/jik19004/miniconda3/envs/CLEAN/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "[W 2024-06-21 14:30:01,030] Trial 378 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m, sampler \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(), study_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStackedLSTMver3\u001b[39m\u001b[38;5;124m\"\u001b[39m, load_if_exists \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqlite:///StackedLSTMver3.db\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(study, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStackedLSTMver3.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m280\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/CLEAN/lib/python3.10/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/CLEAN/lib/python3.10/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/CLEAN/lib/python3.10/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/envs/CLEAN/lib/python3.10/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/envs/CLEAN/lib/python3.10/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      7\u001b[0m params2 \u001b[38;5;241m=\u001b[39m [trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_hiddenZero\u001b[39m\u001b[38;5;124m\"\u001b[39m, low \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m, high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m48\u001b[39m, step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m),\n\u001b[1;32m      8\u001b[0m            trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_hiddenOne\u001b[39m\u001b[38;5;124m\"\u001b[39m, low \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m36\u001b[39m, high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m72\u001b[39m, step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m18\u001b[39m),\n\u001b[1;32m      9\u001b[0m            trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_hiddenTwo\u001b[39m\u001b[38;5;124m\"\u001b[39m, low \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m48\u001b[39m, high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m96\u001b[39m, step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m),\n\u001b[1;32m     10\u001b[0m            trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_hiddenThree\u001b[39m\u001b[38;5;124m\"\u001b[39m, low \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m, high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m64\u001b[39m, step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m),\n\u001b[1;32m     11\u001b[0m            trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_hiddenFour\u001b[39m\u001b[38;5;124m\"\u001b[39m, low \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m, high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m48\u001b[39m, step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m)]\n\u001b[1;32m     12\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m);\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTrain_and_Evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTrainingLoader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mValidationLoader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m260\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 35\u001b[0m, in \u001b[0;36mTrain_and_Evaluate\u001b[0;34m(train_loader, val_loader, device, params1, params2, numEpochs, early_stop_epochs)\u001b[0m\n\u001b[1;32m     33\u001b[0m Optimizer\u001b[38;5;241m.\u001b[39mzero_grad();\n\u001b[1;32m     34\u001b[0m batchLoss \u001b[38;5;241m=\u001b[39m LossFunction(predictedVal, output);\n\u001b[0;32m---> 35\u001b[0m \u001b[43mbatchLoss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m;\n\u001b[1;32m     36\u001b[0m Optimizer\u001b[38;5;241m.\u001b[39mstep();\n\u001b[1;32m     37\u001b[0m Training_Loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batchLoss \u001b[38;5;241m*\u001b[39m output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m#* output.size(0);\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/CLEAN/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/CLEAN/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "def objective(trial):\n",
    "    #num_layers = params[0], outfeatures = params[1], dim_feedforward = params[2]\n",
    "    params1 = [trial.suggest_int(\"num_layers\", low = 2, high = 4, step = 1),\n",
    "              trial.suggest_int(\"LSTM_neurons\", low = 24, high = 48, step = 12)]\n",
    "\n",
    "    params2 = [trial.suggest_int(\"num_hiddenZero\", low = 24, high = 48, step = 12),\n",
    "               trial.suggest_int(\"num_hiddenOne\", low = 36, high = 72, step = 18),\n",
    "               trial.suggest_int(\"num_hiddenTwo\", low = 48, high = 96, step = 24),\n",
    "               trial.suggest_int(\"num_hiddenThree\", low = 32, high = 64, step = 16),\n",
    "               trial.suggest_int(\"num_hiddenFour\", low = 24, high = 48, step = 12)]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\");\n",
    "    return Train_and_Evaluate(TrainingLoader, ValidationLoader, device, params1, params2, 260, 35);\n",
    "\n",
    "import joblib\n",
    "study_name = 'sqlite:///StackedLSTMver2.db'\n",
    "study = optuna.create_study(direction = \"minimize\", sampler = optuna.samplers.TPESampler(), study_name = \"StackedLSTMver3\", load_if_exists = True, storage = 'sqlite:///StackedLSTMver3.db')\n",
    "joblib.dump(study, \"StackedLSTMver2.pkl\")\n",
    "study.optimize(objective, n_trials = 280)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_layers': 3, 'LSTM_neurons': 24, 'num_hiddenZero': 24, 'num_hiddenOne': 36, 'num_hiddenTwo': 48, 'num_hiddenThree': 48, 'num_hiddenFour': 24}\n",
      "123.8967056274414\n"
     ]
    }
   ],
   "source": [
    "print(study.best_params)\n",
    "print(study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed  0 epoch Training Loss:  3026.4539925318377  Validation Loss:  tensor(1879.3081, device='cuda:0')\n",
      "passed  1 epoch Training Loss:  564.3100102727999  Validation Loss:  tensor(165.5255, device='cuda:0')\n",
      "passed  2 epoch Training Loss:  227.62173267891794  Validation Loss:  tensor(183.9295, device='cuda:0')\n",
      "passed  3 epoch Training Loss:  222.7155249726874  Validation Loss:  tensor(174.1448, device='cuda:0')\n",
      "passed  4 epoch Training Loss:  220.37071762845076  Validation Loss:  tensor(163.7190, device='cuda:0')\n",
      "passed  5 epoch Training Loss:  214.79747908751446  Validation Loss:  tensor(168.7150, device='cuda:0')\n",
      "passed  6 epoch Training Loss:  215.3638854012099  Validation Loss:  tensor(162.5921, device='cuda:0')\n",
      "passed  7 epoch Training Loss:  213.62908669917002  Validation Loss:  tensor(160.9086, device='cuda:0')\n",
      "passed  8 epoch Training Loss:  211.6064865393709  Validation Loss:  tensor(164.0545, device='cuda:0')\n",
      "passed  9 epoch Training Loss:  210.2813605752768  Validation Loss:  tensor(161.6371, device='cuda:0')\n",
      "passed  10 epoch Training Loss:  209.00202194791854  Validation Loss:  tensor(148.8120, device='cuda:0')\n",
      "passed  11 epoch Training Loss:  207.76183410243448  Validation Loss:  tensor(161.5065, device='cuda:0')\n",
      "passed  12 epoch Training Loss:  206.22993135160695  Validation Loss:  tensor(151.8464, device='cuda:0')\n",
      "passed  13 epoch Training Loss:  205.2125328158886  Validation Loss:  tensor(154.1079, device='cuda:0')\n",
      "passed  14 epoch Training Loss:  205.65913871541082  Validation Loss:  tensor(160.8594, device='cuda:0')\n",
      "passed  15 epoch Training Loss:  202.26851142237513  Validation Loss:  tensor(158.7287, device='cuda:0')\n",
      "passed  16 epoch Training Loss:  202.81525266195965  Validation Loss:  tensor(154.8666, device='cuda:0')\n",
      "passed  17 epoch Training Loss:  201.203564498508  Validation Loss:  tensor(171.2280, device='cuda:0')\n",
      "passed  18 epoch Training Loss:  199.82286757871736  Validation Loss:  tensor(155.9964, device='cuda:0')\n",
      "passed  19 epoch Training Loss:  199.22673536941315  Validation Loss:  tensor(160.4717, device='cuda:0')\n",
      "passed  20 epoch Training Loss:  197.45774291910578  Validation Loss:  tensor(154.5301, device='cuda:0')\n",
      "passed  21 epoch Training Loss:  197.69773509221062  Validation Loss:  tensor(178.8669, device='cuda:0')\n",
      "passed  22 epoch Training Loss:  195.89301612666526  Validation Loss:  tensor(161.7359, device='cuda:0')\n",
      "passed  23 epoch Training Loss:  193.9238997505177  Validation Loss:  tensor(160.9365, device='cuda:0')\n",
      "passed  24 epoch Training Loss:  193.72035155804133  Validation Loss:  tensor(157.7000, device='cuda:0')\n",
      "passed  25 epoch Training Loss:  192.05817992075268  Validation Loss:  tensor(149.6040, device='cuda:0')\n",
      "passed  26 epoch Training Loss:  192.4671514993396  Validation Loss:  tensor(147.7453, device='cuda:0')\n",
      "passed  27 epoch Training Loss:  190.08650349764378  Validation Loss:  tensor(144.8439, device='cuda:0')\n",
      "passed  28 epoch Training Loss:  189.25556443328387  Validation Loss:  tensor(158.6067, device='cuda:0')\n",
      "passed  29 epoch Training Loss:  187.00407650790027  Validation Loss:  tensor(151.4513, device='cuda:0')\n",
      "passed  30 epoch Training Loss:  185.71497056761297  Validation Loss:  tensor(156.0152, device='cuda:0')\n",
      "passed  31 epoch Training Loss:  185.01550703605264  Validation Loss:  tensor(153.3816, device='cuda:0')\n",
      "passed  32 epoch Training Loss:  184.15081448627848  Validation Loss:  tensor(144.9246, device='cuda:0')\n",
      "passed  33 epoch Training Loss:  184.1139302427968  Validation Loss:  tensor(144.6198, device='cuda:0')\n",
      "passed  34 epoch Training Loss:  181.82686255645964  Validation Loss:  tensor(154.7548, device='cuda:0')\n",
      "passed  35 epoch Training Loss:  180.65633407797543  Validation Loss:  tensor(148.1073, device='cuda:0')\n",
      "passed  36 epoch Training Loss:  178.92239959561041  Validation Loss:  tensor(154.6100, device='cuda:0')\n",
      "passed  37 epoch Training Loss:  176.67888531967975  Validation Loss:  tensor(145.9433, device='cuda:0')\n",
      "passed  38 epoch Training Loss:  177.32737619645508  Validation Loss:  tensor(150.1288, device='cuda:0')\n",
      "passed  39 epoch Training Loss:  176.8010174963719  Validation Loss:  tensor(143.3949, device='cuda:0')\n",
      "passed  40 epoch Training Loss:  175.9864496877395  Validation Loss:  tensor(138.8161, device='cuda:0')\n",
      "passed  41 epoch Training Loss:  171.7143509384121  Validation Loss:  tensor(170.3153, device='cuda:0')\n",
      "passed  42 epoch Training Loss:  172.19363412526295  Validation Loss:  tensor(143.5672, device='cuda:0')\n",
      "passed  43 epoch Training Loss:  170.56635739560062  Validation Loss:  tensor(155.5207, device='cuda:0')\n",
      "passed  44 epoch Training Loss:  169.12436610302152  Validation Loss:  tensor(136.8517, device='cuda:0')\n",
      "passed  45 epoch Training Loss:  167.4222773003734  Validation Loss:  tensor(160.3344, device='cuda:0')\n",
      "passed  46 epoch Training Loss:  166.24905832667503  Validation Loss:  tensor(155.6287, device='cuda:0')\n",
      "passed  47 epoch Training Loss:  164.90946891255075  Validation Loss:  tensor(139.9254, device='cuda:0')\n",
      "passed  48 epoch Training Loss:  163.22212076247004  Validation Loss:  tensor(152.1472, device='cuda:0')\n",
      "passed  49 epoch Training Loss:  161.20912485528396  Validation Loss:  tensor(140.6297, device='cuda:0')\n",
      "passed  50 epoch Training Loss:  160.08588386844295  Validation Loss:  tensor(147.9345, device='cuda:0')\n",
      "passed  51 epoch Training Loss:  159.86464363167937  Validation Loss:  tensor(152.7319, device='cuda:0')\n",
      "passed  52 epoch Training Loss:  157.66528608932444  Validation Loss:  tensor(138.6986, device='cuda:0')\n",
      "passed  53 epoch Training Loss:  156.57317331680989  Validation Loss:  tensor(142.2008, device='cuda:0')\n",
      "passed  54 epoch Training Loss:  156.5465129551421  Validation Loss:  tensor(135.7841, device='cuda:0')\n",
      "passed  55 epoch Training Loss:  155.60350253558792  Validation Loss:  tensor(169.0486, device='cuda:0')\n",
      "passed  56 epoch Training Loss:  154.5916480506139  Validation Loss:  tensor(138.8878, device='cuda:0')\n",
      "passed  57 epoch Training Loss:  154.7468325533615  Validation Loss:  tensor(134.8294, device='cuda:0')\n",
      "passed  58 epoch Training Loss:  152.98975981215452  Validation Loss:  tensor(143.1073, device='cuda:0')\n",
      "passed  59 epoch Training Loss:  152.74844685048998  Validation Loss:  tensor(165.3883, device='cuda:0')\n",
      "passed  60 epoch Training Loss:  151.5436104815171  Validation Loss:  tensor(134.3478, device='cuda:0')\n",
      "passed  61 epoch Training Loss:  150.08702529065502  Validation Loss:  tensor(138.0559, device='cuda:0')\n",
      "passed  62 epoch Training Loss:  150.21506025078676  Validation Loss:  tensor(151.4529, device='cuda:0')\n",
      "passed  63 epoch Training Loss:  148.74618031209744  Validation Loss:  tensor(137.9871, device='cuda:0')\n",
      "passed  64 epoch Training Loss:  149.37983270011577  Validation Loss:  tensor(136.9823, device='cuda:0')\n",
      "passed  65 epoch Training Loss:  147.42904430348787  Validation Loss:  tensor(138.6723, device='cuda:0')\n",
      "passed  66 epoch Training Loss:  147.17406688734164  Validation Loss:  tensor(149.6181, device='cuda:0')\n",
      "passed  67 epoch Training Loss:  146.63249465977466  Validation Loss:  tensor(131.5102, device='cuda:0')\n",
      "passed  68 epoch Training Loss:  144.3323984541882  Validation Loss:  tensor(132.7791, device='cuda:0')\n",
      "passed  69 epoch Training Loss:  145.8412933944266  Validation Loss:  tensor(153.2021, device='cuda:0')\n",
      "passed  70 epoch Training Loss:  144.3098472124839  Validation Loss:  tensor(140.6121, device='cuda:0')\n",
      "passed  71 epoch Training Loss:  144.6601496893701  Validation Loss:  tensor(146.0527, device='cuda:0')\n",
      "passed  72 epoch Training Loss:  143.7801457759225  Validation Loss:  tensor(128.1256, device='cuda:0')\n",
      "passed  73 epoch Training Loss:  142.86586658404943  Validation Loss:  tensor(143.0122, device='cuda:0')\n",
      "passed  74 epoch Training Loss:  142.49311070164853  Validation Loss:  tensor(176.8017, device='cuda:0')\n",
      "passed  75 epoch Training Loss:  142.01596360493747  Validation Loss:  tensor(136.4093, device='cuda:0')\n",
      "passed  76 epoch Training Loss:  141.61661258499518  Validation Loss:  tensor(142.3551, device='cuda:0')\n",
      "passed  77 epoch Training Loss:  140.9043325125964  Validation Loss:  tensor(153.9118, device='cuda:0')\n",
      "passed  78 epoch Training Loss:  140.41968464134882  Validation Loss:  tensor(135.2108, device='cuda:0')\n",
      "passed  79 epoch Training Loss:  139.97330702626903  Validation Loss:  tensor(161.3782, device='cuda:0')\n",
      "passed  80 epoch Training Loss:  139.49134965023563  Validation Loss:  tensor(150.7876, device='cuda:0')\n",
      "passed  81 epoch Training Loss:  139.0376017088721  Validation Loss:  tensor(150.2324, device='cuda:0')\n",
      "passed  82 epoch Training Loss:  138.3862083584718  Validation Loss:  tensor(170.1837, device='cuda:0')\n",
      "passed  83 epoch Training Loss:  138.48084856588451  Validation Loss:  tensor(152.8002, device='cuda:0')\n",
      "passed  84 epoch Training Loss:  138.16291356172647  Validation Loss:  tensor(185.3076, device='cuda:0')\n",
      "passed  85 epoch Training Loss:  137.4970730673276  Validation Loss:  tensor(162.6109, device='cuda:0')\n",
      "passed  86 epoch Training Loss:  137.04728749164315  Validation Loss:  tensor(168.4761, device='cuda:0')\n",
      "passed  87 epoch Training Loss:  137.6390170724151  Validation Loss:  tensor(147.5946, device='cuda:0')\n",
      "passed  88 epoch Training Loss:  136.72989058652794  Validation Loss:  tensor(169.5749, device='cuda:0')\n",
      "passed  89 epoch Training Loss:  136.59621373946223  Validation Loss:  tensor(164.7602, device='cuda:0')\n",
      "passed  90 epoch Training Loss:  136.15005625580903  Validation Loss:  tensor(156.5288, device='cuda:0')\n",
      "passed  91 epoch Training Loss:  136.31378511911555  Validation Loss:  tensor(180.5989, device='cuda:0')\n",
      "passed  92 epoch Training Loss:  135.50229915045574  Validation Loss:  tensor(176.7710, device='cuda:0')\n",
      "passed  93 epoch Training Loss:  135.506049537724  Validation Loss:  tensor(174.0572, device='cuda:0')\n",
      "passed  94 epoch Training Loss:  135.1169387056272  Validation Loss:  tensor(187.5103, device='cuda:0')\n",
      "passed  95 epoch Training Loss:  134.61551192786212  Validation Loss:  tensor(173.5749, device='cuda:0')\n",
      "passed  96 epoch Training Loss:  134.22948293573793  Validation Loss:  tensor(178.4066, device='cuda:0')\n",
      "passed  97 epoch Training Loss:  133.78970926345656  Validation Loss:  tensor(163.2369, device='cuda:0')\n",
      "passed  98 epoch Training Loss:  133.73236095031552  Validation Loss:  tensor(216.5205, device='cuda:0')\n",
      "passed  99 epoch Training Loss:  134.38705627211505  Validation Loss:  tensor(169.7593, device='cuda:0')\n",
      "passed  100 epoch Training Loss:  132.73704893440083  Validation Loss:  tensor(209.3146, device='cuda:0')\n",
      "passed  101 epoch Training Loss:  132.69490599572782  Validation Loss:  tensor(220.3567, device='cuda:0')\n",
      "passed  102 epoch Training Loss:  132.57678510280954  Validation Loss:  tensor(192.8283, device='cuda:0')\n",
      "passed  103 epoch Training Loss:  131.99203450356288  Validation Loss:  tensor(175.7514, device='cuda:0')\n",
      "passed  104 epoch Training Loss:  132.07901087612308  Validation Loss:  tensor(194.1081, device='cuda:0')\n",
      "passed  105 epoch Training Loss:  131.90522934433446  Validation Loss:  tensor(177.7234, device='cuda:0')\n",
      "passed  106 epoch Training Loss:  131.31080111533257  Validation Loss:  tensor(232.0513, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(128.1256, device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params1 = [study.best_params[\"num_layers\"], study.best_params[\"LSTM_neurons\"]]\n",
    "params2 = [study.best_params[\"num_hiddenZero\"], study.best_params[\"num_hiddenOne\"], study.best_params[\"num_hiddenTwo\"], study.best_params[\"num_hiddenThree\"], study.best_params[\"num_hiddenFour\"]]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\");\n",
    "\n",
    "Train_and_Evaluate(TrainingLoader, ValidationLoader, device, params1, params2, 260, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.056055516\n",
      "170.5203\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "model = torch.load(\"/home/jik19004/FilesToRun/StackedEnsemblePred/StackedLSTMver3BEST\")\n",
    "values = predict(model, TestingLoader, torch.device(\"cuda\"))\n",
    "\n",
    "error = mean_absolute_percentage_error(values[1], values[0])\n",
    "error2 = mean_absolute_error(values[1], values[0])\n",
    "print(error)\n",
    "print(error2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.038251944\n",
      "128.12563\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"/home/jik19004/FilesToRun/StackedEnsemblePred/StackedLSTMver3BEST\")\n",
    "values = predict(model, ValidationLoader, torch.device(\"cuda\"))\n",
    "\n",
    "error = mean_absolute_percentage_error(values[1], values[0])\n",
    "print(error)\n",
    "error = mean_absolute_error(values[1], values[0])\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0656502\n",
      "232.51476\n"
     ]
    }
   ],
   "source": [
    "values = predict(model, TrainingLoader, torch.device(\"cuda\"))\n",
    "\n",
    "error = mean_absolute_percentage_error(values[1], values[0])\n",
    "print(error)\n",
    "error = mean_absolute_error(values[1], values[0])\n",
    "print(error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_layers': 4, 'last_hidden_neurons': 42, 'LSTM_neurons': 32, 'act_slope': 0.10339207803590009, 'num_hiddenZero': 36, 'num_hiddenOne': 72, 'num_hiddenTwo': 48, 'num_hiddenThree': 64, 'num_hiddenFour': 48}\n"
     ]
    }
   ],
   "source": [
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLEAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
