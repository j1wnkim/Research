{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "uWNK0BtB2W0E"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch import nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>BDL_tmpc</th>\n",
       "      <th>BDR_tmpc</th>\n",
       "      <th>DXR_tmpc</th>\n",
       "      <th>GON_tmpc</th>\n",
       "      <th>HFD_tmpc</th>\n",
       "      <th>HVN_tmpc</th>\n",
       "      <th>IJD_tmpc</th>\n",
       "      <th>MMK_tmpc</th>\n",
       "      <th>OXC_tmpc</th>\n",
       "      <th>SNC_tmpc</th>\n",
       "      <th>Demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/1/2011 0:00</td>\n",
       "      <td>1.11</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.78</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.11</td>\n",
       "      <td>-2.78</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3053.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/1/2011 1:00</td>\n",
       "      <td>1.11</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>3.33</td>\n",
       "      <td>2.22</td>\n",
       "      <td>3.33</td>\n",
       "      <td>-2.78</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2892.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/1/2011 2:00</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>2.78</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>3.33</td>\n",
       "      <td>2.78</td>\n",
       "      <td>2.78</td>\n",
       "      <td>-2.22</td>\n",
       "      <td>0.56</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2774.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/1/2011 3:00</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>3.89</td>\n",
       "      <td>2.22</td>\n",
       "      <td>1.11</td>\n",
       "      <td>-2.78</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2710.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/1/2011 4:00</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>1.67</td>\n",
       "      <td>-1.11</td>\n",
       "      <td>3.33</td>\n",
       "      <td>2.22</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>-2.78</td>\n",
       "      <td>-1.67</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2698.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Datetime  BDL_tmpc  BDR_tmpc  DXR_tmpc  GON_tmpc  HFD_tmpc  HVN_tmpc  \\\n",
       "0  1/1/2011 0:00      1.11      1.11      0.00      2.78      1.67      1.11   \n",
       "1  1/1/2011 1:00      1.11      2.22     -0.56      3.33      2.22      3.33   \n",
       "2  1/1/2011 2:00     -0.56      2.78     -1.67      3.33      2.78      2.78   \n",
       "3  1/1/2011 3:00     -1.11      2.22     -1.11      3.89      2.22      1.11   \n",
       "4  1/1/2011 4:00     -1.67      1.67     -1.11      3.33      2.22     -0.56   \n",
       "\n",
       "   IJD_tmpc  MMK_tmpc  OXC_tmpc  SNC_tmpc  Demand  \n",
       "0     -2.78     -1.67       3.0       2.0  3053.0  \n",
       "1     -2.78     -1.67       3.0       3.0  2892.0  \n",
       "2     -2.22      0.56       3.0       3.0  2774.0  \n",
       "3     -2.78      0.56       2.0       4.0  2710.0  \n",
       "4     -2.78     -1.67       4.0       5.0  2698.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"/home/jik19004/ASOS_10_CT_stations_tmpc_demand_2011_2023.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "WDYp3nRh-Bpv"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime \n",
    "\n",
    "WeatherData = pd.read_csv('/home/jik19004/ASOS_10_CT_stations_tmpc_demand_2011_2023.csv').drop(columns = [\"Unnamed: 0\"])\n",
    "WeatherData.ffill(inplace = True)\n",
    "WeatherData.bfill(inplace = True) # fill in missing values with the previous value.\n",
    "DateTimeCol = WeatherData[\"Datetime\"]\n",
    "HourCol = []\n",
    "WeekDayorWeekEndCol = [] \n",
    "\n",
    "for date in DateTimeCol:\n",
    "    date = datetime.strptime(date, \"%m/%d/%Y %H:%M\")\n",
    "    HourCol.append(date.hour)\n",
    "    if date.weekday() < 5:\n",
    "        WeekDayorWeekEndCol.append(0)\n",
    "    else:\n",
    "        WeekDayorWeekEndCol.append(1)\n",
    "\n",
    "WeatherData.drop(columns = [\"Datetime\"], inplace = True) # drop the datetime column. \n",
    "\n",
    "\n",
    "WeatherData.insert(0, \"Hour\", HourCol)\n",
    "WeatherData.insert(1, \"Weekday or Weekend\", WeekDayorWeekEndCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index for Jan 1, 2020:  78883\n",
      "index for Jan 1, 2021:  87667\n",
      "index for Dec 31, 2022  105186\n"
     ]
    }
   ],
   "source": [
    "DateTimeCol = [datetime.strptime(date, \"%m/%d/%Y %H:%M\") for date in DateTimeCol]\n",
    "for i in range(len(DateTimeCol)):\n",
    "    date = DateTimeCol[i]\n",
    "    if int(date.year) == 2020 and int(date.month) == 1 and int(date.day) == 1 and int(date.hour) == 0: # end of training \n",
    "        print(\"index for Jan 1, 2020: \", i)\n",
    "    if int(date.year) == 2021 and int(date.month) == 1 and int(date.day) == 1 and int(date.hour) == 0: # start of validation\n",
    "        print(\"index for Jan 1, 2021: \", i)\n",
    "    if int(date.year) == 2022 and int(date.month) == 12 and int(date.day) == 31 and int(date.hour) == 23: # end of validation \n",
    "        print(\"index for Dec 31, 2022 \", i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add +1 to those indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "YGFZFPjYGyOK"
   },
   "outputs": [],
   "source": [
    "def return_sequences(data, outputData, input_n_steps, output_n_steps):\n",
    "    X = []\n",
    "    Y = []\n",
    "    length = len(data)\n",
    "    for i in range(0,length, 1):\n",
    "        input_indx = i + input_n_steps\n",
    "        output_indx = input_indx + output_n_steps\n",
    "        if (output_indx > len(data)): # we need to have equally split sequences.\n",
    "            break               # The remaining data that cannot fit into a fixed\n",
    "                                # sequence will immediately be cut!\n",
    "        else:\n",
    "            Xsample = data.iloc[i:input_indx, :] # get the previous data\n",
    "            Ysample = outputData[input_indx:output_indx]\n",
    "            X.append(Xsample)\n",
    "            Y.append(Ysample)\n",
    "    X = np.asarray(X).astype('float64')\n",
    "    Y = np.asarray(Y).astype('float64')\n",
    "    return (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ewjMMr81GyOK"
   },
   "outputs": [],
   "source": [
    "def splitDataAndScale(data, output, split_1 = 78883, split_2 = 87667, split_3 = 105187):\n",
    "    TrainingData = (data.iloc[:split_1, :].copy())\n",
    "    TrainingCategories = TrainingData.iloc[:, [0,1]]\n",
    "    TrainingNumerical = TrainingData.iloc[:, 2:]\n",
    "    TrainingOutput = output[:split_1].copy()  \n",
    "    Scaler = StandardScaler().fit(TrainingNumerical)\n",
    "    TrainingNumerical = Scaler.transform(TrainingNumerical)\n",
    "    TrainingData = pd.concat([TrainingCategories, pd.DataFrame(TrainingNumerical)], axis = 1)\n",
    "    \n",
    "    ValidationData = data.iloc[split_2:split_3, :].copy()\n",
    "    ValidationData.reset_index(drop = True, inplace = True)\n",
    "    ValidationCategories = ValidationData.iloc[:, [0,1]]\n",
    "    ValidationNumerical = ValidationData.iloc[:, 2:]\n",
    "    ValidationNumerical = Scaler.transform(ValidationNumerical)\n",
    "    ValidationData = pd.concat([ValidationCategories, pd.DataFrame(ValidationNumerical)], axis = 1)\n",
    "    ValidationOutput = output[split_2:split_3].copy()\n",
    "    \n",
    "    TestingData = data.iloc[split_3:, :].copy()\n",
    "    TestingData.reset_index(drop = True, inplace = True)\n",
    "    TestingCategories = TestingData.iloc[:, [0,1]]\n",
    "    TestingNumerical = TestingData.iloc[:, 2:]\n",
    "    TestingNumerical = Scaler.transform(TestingNumerical)\n",
    "    TestingData = pd.concat([TestingCategories, pd.DataFrame(TestingNumerical)], axis = 1)\n",
    "    TestingOutput = output[split_3:].copy()\n",
    "\n",
    "\n",
    "    TrainingSequences = return_sequences(TrainingData, TrainingOutput, 18, 1)\n",
    "\n",
    "    TransformedTrainingData = TrainingSequences[0]\n",
    "    TransformedTrainingOutput = TrainingSequences[1]\n",
    "\n",
    "    ValidationSequences = return_sequences(ValidationData, ValidationOutput, 18, 1)\n",
    "\n",
    "    TransformedValidationData = ValidationSequences[0]\n",
    "    TransformedValidationOutput = ValidationSequences[1]\n",
    "\n",
    "    TestingSequences = return_sequences(TestingData, TestingOutput, 18, 1)\n",
    "\n",
    "    TransformedTestingData = TestingSequences[0]\n",
    "    TransformedTestingOutput = TestingSequences[1]\n",
    "\n",
    "\n",
    "    return (TransformedTrainingData, TransformedTrainingOutput, TransformedValidationData, TransformedValidationOutput,\n",
    "    TransformedTestingData, TransformedTestingOutput)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8BJQAqaJqv7"
   },
   "source": [
    "## Time Series Transformations\n",
    "\n",
    "1. The dataset is to be re-sampled at an hourly rate for more meaningful analytics.\n",
    "\n",
    "2. To alleviate exponential effects, the target variable is log-transformed as per the Uber paper.\n",
    "\n",
    "3. For simplicity and speed when running this notebook, only temporal and autoregressive features, namely `day_of_week`, `hour_of_day`, \\\n",
    "and previous values of `Appliances` are used as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFnQ3txg_2lj"
   },
   "source": [
    "# Prepare Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gUa6m83NgDn"
   },
   "source": [
    "For this example, we will use sliding windows of 10 points per each window (equivalent to 10 hours) to predict each next point. The window size can be altered via the `sequence_length` variable.\n",
    "\n",
    "Min-Max scaling has also been fitted to the training data to aid the convergence of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "JSuQZkdKGyOL"
   },
   "outputs": [],
   "source": [
    "DemandData = WeatherData['Demand'].copy() # The output data\n",
    "WeatherData.drop(columns = ['Demand'], inplace = True)\n",
    "data = splitDataAndScale(WeatherData, DemandData) # splitting the data into training, validation, and testing.\n",
    "\n",
    "\n",
    "TrainingData = data[0]\n",
    "TrainingOutput = data[1]\n",
    "\n",
    "ValidationData = data[2]\n",
    "ValidationOutput = data[3]\n",
    "\n",
    "TestingData = data[4]\n",
    "TestingOutput = data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "_RgEVrgWGyOL",
    "outputId": "f5305ed4-349a-4b81-b714-549f428c4e21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78865, 1)\n",
      "(17502, 1)\n",
      "(8742, 1)\n"
     ]
    }
   ],
   "source": [
    "print(TrainingOutput.shape)\n",
    "print(ValidationOutput.shape)\n",
    "print(TestingOutput.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78865, 18, 12)\n",
      "(17502, 18, 12)\n",
      "(8742, 18, 12)\n"
     ]
    }
   ],
   "source": [
    "print(TrainingData.shape)\n",
    "print(ValidationData.shape)\n",
    "print(TestingData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78865, 18, 12)\n"
     ]
    }
   ],
   "source": [
    "print(TrainingData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ue1JcZm_bgsd"
   },
   "source": [
    "# Define Bayesian LSTM Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNag-wa-04WZ"
   },
   "source": [
    "To demonstrate a simple working example of the Bayesian LSTM, a model with a similar architecture and size to that in Uber's paper has been used a starting point. The network architecture is as follows:\n",
    "\n",
    "Encoder-Decoder Stage:\n",
    " - A uni-directional LSTM with 2 stacked layers & 128 hidden units acting as an encoding layer to construct a fixed-dimension embedding state\n",
    " - A uni-directional LSTM with 2 stacked layers & 32 hidden units acting as a decoding layer to produce predictions at future steps\n",
    " - Dropout is applied at **both** training and inference for both LSTM layers\n",
    "\n",
    "\n",
    " Predictor Stage:\n",
    " - 1 fully-connected output layer with 1 output (for predicting the target value) to produce a single value for the target variable\n",
    "\n",
    "\n",
    "By allowing dropout at both training and testing time, the model simulates random sampling, thus allowing varying predictions that can be used to estimate the underlying distribution of the target value, enabling explicit model uncertainties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "OgWyOffPbO0b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "class BaselineLSTM(torch.nn.Module):\n",
    "    def __init__(self, LSTMNeurons1, LSTMNeurons2, num_layers, params, output_num =1):\n",
    "        super(BaselineLSTM, self).__init__()\n",
    "        self.LSTM1 = torch.nn.LSTM(input_size = 12, hidden_size = LSTMNeurons1, num_layers = 1, bias = True, batch_first = True, bidirectional = False)\n",
    "        self.LSTM2 = torch.nn.LSTM(input_size = LSTMNeurons1, hidden_size = LSTMNeurons2, num_layers = 1, bias = True, batch_first = True, bidirectional = False)\n",
    "\n",
    "        #self.batchNorm0 = torch.nn.BatchNorm1d(num_features = LSTMNeurons)\n",
    "        input_size = LSTMNeurons2\n",
    "        layers = [] \n",
    "        for i in range(0,num_layers):\n",
    "            num_units = params[i]\n",
    "            layers.append(torch.nn.Linear(input_size, num_units, bias = True))\n",
    "            if i!= num_layers -1:\n",
    "                layers.append(torch.nn.BatchNorm1d(num_units))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            layers.append(torch.nn.Dropout(0.2))\n",
    "            input_size = num_units\n",
    "        self.intermediateLayers = torch.nn.Sequential(*layers)\n",
    "        self.Linear1 = torch.nn.Linear(in_features = input_size, out_features = output_num, bias = True)\n",
    "\n",
    "        \n",
    "    def forward(self, val):\n",
    "        \n",
    "        x = self.LSTM1(val)\n",
    "        x = self.LSTM2(x[0]) \n",
    "\n",
    "        x = x[0]\n",
    "        x = x[:, -1, :]\n",
    "        \n",
    "        x = self.intermediateLayers(x)\n",
    "        x = self.Linear1(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQ8JLm-ShlaU"
   },
   "source": [
    "### Begin Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "015pu48r3X1F"
   },
   "source": [
    "To train the Bayesian LSTM, we use the ADAM optimizer along with mini-batch gradient descent (`batch_size = 128`). For quick demonstration purposes, the model is trained for 150 epochs.\n",
    "\n",
    "The Bayesian LSTM is trained on the first 70% of data points, using the aforementioned sliding windows of size 10. The remaining 30% of the dataset is held out purely for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7iZ__nxaCzZE",
    "outputId": "915e737a-6235-4842-b7fe-e94648a86cfb"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, output):\n",
    "        data = torch.tensor(data).float();\n",
    "        output = torch.tensor(output).float()\n",
    "        self.data = data\n",
    "        self.output = output;\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx];\n",
    "        y = self.output[idx];\n",
    "\n",
    "        return x, y;\n",
    "\n",
    "# use the past 72 hours in advance and then predict the 1st hour, 6th hour, 12 hours!\n",
    "\n",
    "def evaluate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            running_loss += loss.item() * target.size(0)\n",
    "    return running_loss / len(val_loader.dataset)\n",
    "\n",
    "def Train_and_Evaluate(train_loader, val_loader, device, params1, params2, numEpochs, early_stop_epochs):\n",
    "    model = BaselineLSTM(params1 = params1, params2 = params2, device = device)\n",
    "    model = model.to(device);\n",
    "    LossFunction = torch.nn.L1Loss();\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "\n",
    "\n",
    "    Optimizer = torch.optim.Adam(params = model.parameters())\n",
    "    for epoch in range(0,numEpochs):\n",
    "        model.train()\n",
    "        Training_Loss = 0;\n",
    "        total_samples = 0;\n",
    "        for input, output in train_loader:\n",
    "            input = input.to(device);\n",
    "            print(input.shape)\n",
    "            output = torch.squeeze(output, 1);\n",
    "            output = output.to(device);\n",
    "            predictedVal = model(input)\n",
    "            predictedVal = torch.squeeze(predictedVal, 1)\n",
    "            Optimizer.zero_grad();\n",
    "            batchLoss = LossFunction(predictedVal, output);\n",
    "            batchLoss.backward();\n",
    "            Optimizer.step();\n",
    "            Training_Loss += batchLoss * output.size(0) #* output.size(0);\n",
    "            total_samples += output.size(0)\n",
    "        Training_Loss = Training_Loss.item()/total_samples\n",
    "\n",
    "\n",
    "        Validation_Loss = 0;\n",
    "        print(\"passed \", epoch, \"epoch\", \"Training Loss: \", Training_Loss,\" \", end = \"\")\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            total_val_samples = 0;\n",
    "            Validation_Loss = 0;\n",
    "            for val_input, val_output in val_loader:\n",
    "                val_input = val_input.to(device);\n",
    "                val_output = torch.squeeze(val_output,1);\n",
    "                val_output = val_output.to(device);\n",
    "                predictedVal = model(val_input)\n",
    "                predictedVal = torch.squeeze(predictedVal, 1)\n",
    "                Validation_Loss += LossFunction(val_output, predictedVal) * val_output.size(0)\n",
    "                total_val_samples += val_output.size(0)\n",
    "            Validation_Loss = Validation_Loss/total_val_samples\n",
    "            print(\"Validation Loss: \", Validation_Loss)\n",
    "\n",
    "            if Validation_Loss < best_val_loss:\n",
    "                best_val_loss = Validation_Loss\n",
    "                torch.save(model, \"/home/jik19004/FilesToRun/BaselineLSTM20/BaselineLSTM20\")\n",
    "                early_stop_count = 0;   \n",
    "            else:\n",
    "                early_stop_count +=1\n",
    "            if early_stop_count >= early_stop_epochs:\n",
    "                return best_val_loss;\n",
    "    return best_val_loss;\n",
    "\n",
    "def predict(model, data_loader, device):\n",
    "    #model.eval()\n",
    "    predictions = []\n",
    "    act_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for data, _ in data_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            predictions.append(output.cpu().numpy())\n",
    "            act_outputs.append(_.numpy())\n",
    "\n",
    "    return (np.concatenate(predictions), np.concatenate(act_outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_and_Evaluate2(train_loader, val_loader, device, params1, params2, numEpochs, early_stop_epochs):\n",
    "    #num_layers, input_dim, hidden_unit1, hidden_unit2, output_unit, lastNeurons, batch_size, params, device = None\n",
    "    model = BaselineLSTM(LSTMNeurons1 = params1[0], LSTMNeurons2 = params1[1], num_layers = params1[2], params = params2)\n",
    "    model = model.to(device);\n",
    "    TrainEpochLoss = [] \n",
    "    ValidationEpochLoss = [] \n",
    "    LossFunction = torch.nn.L1Loss();\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_count = 0\n",
    "\n",
    "\n",
    "    Optimizer = torch.optim.Adam(params = model.parameters())\n",
    "    for epoch in range(0,numEpochs):\n",
    "        model.train()\n",
    "        Training_Loss = 0;\n",
    "        total_samples = 0;\n",
    "        for input, output in train_loader:\n",
    "            input = input.to(device);\n",
    "            output = torch.squeeze(output, 1);\n",
    "            output = output.to(device);\n",
    "            predictedVal = model(input)\n",
    "            predictedVal = torch.squeeze(predictedVal, 1)\n",
    "            Optimizer.zero_grad();\n",
    "            batchLoss = LossFunction(predictedVal, output);\n",
    "            batchLoss.backward();\n",
    "            Optimizer.step();\n",
    "            Training_Loss += batchLoss * output.size(0) #* output.size(0);\n",
    "            total_samples += output.size(0)\n",
    "        Training_Loss = Training_Loss.item()/total_samples\n",
    "        TrainEpochLoss.append(Training_Loss)\n",
    "\n",
    "\n",
    "        Validation_Loss = 0;\n",
    "        print(\"passed \", epoch, \"epoch\", \"Training Loss: \", Training_Loss,\" \", end = \"\")\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            total_val_samples = 0;\n",
    "            Validation_Loss = 0;\n",
    "            for val_input, val_output in val_loader:\n",
    "                val_input = val_input.to(device);\n",
    "                val_output = torch.squeeze(val_output,1);\n",
    "                val_output = val_output.to(device);\n",
    "                predictedVal = model(val_input)\n",
    "                predictedVal = torch.squeeze(predictedVal, 1)\n",
    "                Validation_Loss += LossFunction(val_output, predictedVal) * val_output.size(0)\n",
    "                total_val_samples += val_output.size(0)\n",
    "            Validation_Loss = Validation_Loss.item()/total_val_samples\n",
    "            print(\"Validation Loss: \", Validation_Loss)\n",
    "            ValidationEpochLoss.append(Validation_Loss)\n",
    "\n",
    "            if Validation_Loss < best_val_loss:\n",
    "                best_val_loss = Validation_Loss\n",
    "                torch.save(model, \"/home/jik19004/FilesToRun/BaselineLSTM20/BaselineLSTM20\")\n",
    "                early_stop_count = 0;   \n",
    "            else:\n",
    "                early_stop_count +=1\n",
    "            if early_stop_count >= early_stop_epochs:\n",
    "                return (TrainEpochLoss, ValidationEpochLoss);\n",
    "    return (TrainEpochLoss, ValidationEpochLoss);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "M9m5DLiAGyOO"
   },
   "outputs": [],
   "source": [
    "TrainingData = TimeSeriesDataset(np.array(TrainingData),np.array(TrainingOutput));\n",
    "TrainingLoader = DataLoader(TrainingData, batch_size = 256);\n",
    "\n",
    "\n",
    "ValidationData = TimeSeriesDataset(ValidationData, ValidationOutput); ### Set it with the previous validation data\n",
    "ValidationLoader = DataLoader(ValidationData, batch_size = 256);\n",
    "\n",
    "\n",
    "TestingData = TimeSeriesDataset(TestingData,TestingOutput); ### Set it with the previous testing data.\n",
    "TestingLoader = DataLoader(TestingData, batch_size = 256);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "zCrnRRfnGyOO",
    "outputId": "2d512125-f70e-42f3-c296-a09c4c23bb80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-08-17 11:43:43,424] Using an existing study with name 'NewLSTMBaseline20' instead of creating a new one.\n"
     ]
    }
   ],
   "source": [
    "#num_layers, hidden_unit1, hidden_unit2, lastNeurons, batch_size, params, device = None\n",
    "def objective(trial):\n",
    "    params1 = [trial.suggest_int(\"LSTM_neurons1\", low = 158, high = 256, step = 14),\n",
    "               trial.suggest_int(\"LSTM_neurons2\", low = 158, high = 256, step = 14),\n",
    "              trial.suggest_int(\"num_layers\", low = 1, high = 4, step = 1)]\n",
    "    params2 = [trial.suggest_int(\"num_hiddenZero\", low = 72, high = 180, step = 36),\n",
    "               trial.suggest_int(\"num_hiddenOne\", low = 72, high = 180, step = 36),\n",
    "               trial.suggest_int(\"num_hiddenTwo\", low = 58, high = 154, step = 24),\n",
    "               trial.suggest_int(\"num_hiddenThree\", low = 58, high = 154, step = 24)]\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\");\n",
    "    return Train_and_Evaluate(TrainingLoader, ValidationLoader, device, params1, params2, 2000, 150); \n",
    "\n",
    "\n",
    "import joblib\n",
    "study_name = 'sqlite:///LSTMBaselineOutput20.db'\n",
    "study = optuna.create_study(direction = \"minimize\", sampler = optuna.samplers.TPESampler(), study_name = \"NewLSTMBaseline20\", load_if_exists = True, storage = 'sqlite:///LSTMBaselineOutput20.db')\n",
    "#study.optimize(objective, n_trials = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Qp9s43t4GyOO",
    "outputId": "9a51511e-f87d-40b9-a436-28cf7ddf452e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LSTM_neurons1': 228, 'LSTM_neurons2': 158, 'num_layers': 1, 'num_hiddenZero': 108, 'num_hiddenOne': 180, 'num_hiddenTwo': 82, 'num_hiddenThree': 58}\n",
      "121.05099487304688\n"
     ]
    }
   ],
   "source": [
    "print(study.best_params)\n",
    "print(study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passed  0 epoch Training Loss:  2687.405160717682  Validation Loss:  509.7601416980917\n",
      "passed  1 epoch Training Loss:  519.2107018322449  Validation Loss:  386.9626042737973\n",
      "passed  2 epoch Training Loss:  461.9516388765612  Validation Loss:  286.5111987201463\n",
      "passed  3 epoch Training Loss:  446.13549736892156  Validation Loss:  254.67586561535825\n",
      "passed  4 epoch Training Loss:  434.1878399797122  Validation Loss:  379.09596046166155\n",
      "passed  5 epoch Training Loss:  432.4360869840867  Validation Loss:  263.42823677294024\n",
      "passed  6 epoch Training Loss:  401.269384391048  Validation Loss:  248.53705290823905\n",
      "passed  7 epoch Training Loss:  384.05865719901095  Validation Loss:  287.00331390698204\n",
      "passed  8 epoch Training Loss:  359.1691117732835  Validation Loss:  197.87185750199978\n",
      "passed  9 epoch Training Loss:  367.4473594116528  Validation Loss:  242.20677636841503\n",
      "passed  10 epoch Training Loss:  333.7158435300831  Validation Loss:  199.32859101816936\n",
      "passed  11 epoch Training Loss:  330.2589234768275  Validation Loss:  227.16949491486687\n",
      "passed  12 epoch Training Loss:  320.2104101946364  Validation Loss:  196.78809564621187\n",
      "passed  13 epoch Training Loss:  317.4792366702593  Validation Loss:  202.93662152896812\n",
      "passed  14 epoch Training Loss:  317.3784061370697  Validation Loss:  247.302622557422\n",
      "passed  15 epoch Training Loss:  312.0837126735561  Validation Loss:  180.2837961375843\n",
      "passed  16 epoch Training Loss:  343.5767957902745  Validation Loss:  216.9334790309679\n",
      "passed  17 epoch Training Loss:  319.14854498193114  Validation Loss:  253.67520854759456\n",
      "passed  18 epoch Training Loss:  315.40899004628164  Validation Loss:  217.61500114272656\n",
      "passed  19 epoch Training Loss:  304.7283585874596  Validation Loss:  238.81299280082277\n",
      "passed  20 epoch Training Loss:  302.7315285614658  Validation Loss:  182.15616786652953\n",
      "passed  21 epoch Training Loss:  305.77599695682494  Validation Loss:  196.67763684150384\n",
      "passed  22 epoch Training Loss:  298.0901794205287  Validation Loss:  214.43663581304995\n",
      "passed  23 epoch Training Loss:  301.1789260128067  Validation Loss:  184.22864529768026\n",
      "passed  24 epoch Training Loss:  307.7211690864135  Validation Loss:  203.98787281453548\n",
      "passed  25 epoch Training Loss:  310.6114753059025  Validation Loss:  177.7717832247743\n",
      "passed  26 epoch Training Loss:  294.0413871806251  Validation Loss:  194.27186892926522\n",
      "passed  27 epoch Training Loss:  293.6  Validation Loss:  187.6910638784139\n",
      "passed  28 epoch Training Loss:  292.3833893362074  Validation Loss:  184.71913209918867\n",
      "passed  29 epoch Training Loss:  293.45349648132884  Validation Loss:  185.7077905382242\n",
      "passed  30 epoch Training Loss:  292.6559563811577  Validation Loss:  188.9195234830305\n",
      "passed  31 epoch Training Loss:  291.38337665631144  Validation Loss:  192.32404868015084\n",
      "passed  32 epoch Training Loss:  287.68903822988653  Validation Loss:  190.90326819792023\n",
      "passed  33 epoch Training Loss:  288.95299562543585  Validation Loss:  162.12027196891785\n",
      "passed  34 epoch Training Loss:  288.42982311545046  Validation Loss:  176.00927036910068\n",
      "passed  35 epoch Training Loss:  287.1674380270082  Validation Loss:  186.17589418352188\n",
      "passed  36 epoch Training Loss:  284.60181322513154  Validation Loss:  173.20706205005143\n",
      "passed  37 epoch Training Loss:  287.6094084828504  Validation Loss:  177.39339789738315\n",
      "passed  38 epoch Training Loss:  281.848982438344  Validation Loss:  170.15358244772025\n",
      "passed  39 epoch Training Loss:  290.38843593482534  Validation Loss:  206.06003599588618\n",
      "passed  40 epoch Training Loss:  286.2009256324098  Validation Loss:  199.55273683007655\n",
      "passed  41 epoch Training Loss:  280.7161351676916  Validation Loss:  179.31572106045024\n",
      "passed  42 epoch Training Loss:  279.9124326380524  Validation Loss:  187.70639069820592\n",
      "passed  43 epoch Training Loss:  284.2093958029544  Validation Loss:  175.2454719460633\n",
      "passed  44 epoch Training Loss:  280.7911240727826  Validation Loss:  181.63344189235517\n",
      "passed  45 epoch Training Loss:  285.7333671463894  Validation Loss:  209.51434121814648\n",
      "passed  46 epoch Training Loss:  288.6798453052685  Validation Loss:  195.54946577533997\n",
      "passed  47 epoch Training Loss:  281.0146452799087  Validation Loss:  184.05089418352188\n",
      "passed  48 epoch Training Loss:  277.69809167564824  Validation Loss:  157.39062678551022\n",
      "passed  49 epoch Training Loss:  274.6956190959234  Validation Loss:  177.72908810421666\n",
      "passed  50 epoch Training Loss:  273.48121473403916  Validation Loss:  163.23835847331733\n",
      "passed  51 epoch Training Loss:  275.14732771191274  Validation Loss:  162.49137241458118\n",
      "passed  52 epoch Training Loss:  272.2249413554809  Validation Loss:  168.77999657182036\n",
      "passed  53 epoch Training Loss:  275.6782603182654  Validation Loss:  174.5682921951777\n",
      "passed  54 epoch Training Loss:  272.60546503518674  Validation Loss:  198.02038338475603\n",
      "passed  55 epoch Training Loss:  275.7513725987447  Validation Loss:  192.66226716946636\n",
      "passed  56 epoch Training Loss:  273.11578013060296  Validation Loss:  155.61502971089018\n",
      "passed  57 epoch Training Loss:  273.71236923857225  Validation Loss:  185.01715518226487\n",
      "passed  58 epoch Training Loss:  271.1998985608318  Validation Loss:  160.21091875214262\n",
      "passed  59 epoch Training Loss:  270.64084194509604  Validation Loss:  187.77626842646555\n",
      "passed  60 epoch Training Loss:  273.55394661763773  Validation Loss:  217.81543537881385\n",
      "passed  61 epoch Training Loss:  274.1962594306727  Validation Loss:  151.0450234258942\n",
      "passed  62 epoch Training Loss:  270.3182146706397  Validation Loss:  182.32703405325105\n",
      "passed  63 epoch Training Loss:  269.97027832371776  Validation Loss:  161.94793452176893\n",
      "passed  64 epoch Training Loss:  271.0053889558106  Validation Loss:  163.63008513312764\n",
      "passed  65 epoch Training Loss:  269.4088886071134  Validation Loss:  147.4494772026054\n",
      "passed  66 epoch Training Loss:  270.5334685855576  Validation Loss:  150.32597703119643\n",
      "passed  67 epoch Training Loss:  269.3293856590376  Validation Loss:  169.42603702434008\n",
      "passed  68 epoch Training Loss:  270.94422113738665  Validation Loss:  159.5862901382699\n",
      "passed  69 epoch Training Loss:  274.0054269954986  Validation Loss:  182.15485373100216\n",
      "passed  70 epoch Training Loss:  271.9961199518164  Validation Loss:  167.1085733059079\n",
      "passed  71 epoch Training Loss:  265.17763266341217  Validation Loss:  176.91856644954862\n",
      "passed  72 epoch Training Loss:  267.4295568376339  Validation Loss:  169.00131413552737\n",
      "passed  73 epoch Training Loss:  266.4002028783364  Validation Loss:  147.39081247857388\n",
      "passed  74 epoch Training Loss:  266.73662587966777  Validation Loss:  168.21323277339732\n",
      "passed  75 epoch Training Loss:  266.3960185126482  Validation Loss:  168.99191520969032\n",
      "passed  76 epoch Training Loss:  263.9201673746275  Validation Loss:  154.15313964118386\n",
      "passed  77 epoch Training Loss:  266.6705636213783  Validation Loss:  163.73751571249\n",
      "passed  78 epoch Training Loss:  266.21543143346224  Validation Loss:  161.98601588389897\n",
      "passed  79 epoch Training Loss:  264.9047613009573  Validation Loss:  187.8963404182379\n",
      "passed  80 epoch Training Loss:  268.1078044760033  Validation Loss:  185.08069077819678\n",
      "passed  81 epoch Training Loss:  265.9592975337602  Validation Loss:  186.1370414809736\n",
      "passed  82 epoch Training Loss:  266.3448424522919  Validation Loss:  167.15245400525654\n",
      "passed  83 epoch Training Loss:  264.6126418563368  Validation Loss:  163.72720260541652\n",
      "passed  84 epoch Training Loss:  264.3159069295632  Validation Loss:  164.4810450234259\n",
      "passed  85 epoch Training Loss:  264.0706016610664  Validation Loss:  163.90896754656612\n",
      "passed  86 epoch Training Loss:  263.705116338046  Validation Loss:  175.44289224088675\n",
      "passed  87 epoch Training Loss:  262.00633994801245  Validation Loss:  150.98478745286252\n",
      "passed  88 epoch Training Loss:  263.31485449819314  Validation Loss:  173.7312592846532\n",
      "passed  89 epoch Training Loss:  264.0065935459329  Validation Loss:  144.38104216660952\n",
      "passed  90 epoch Training Loss:  262.695416217587  Validation Loss:  151.2838104216661\n",
      "passed  91 epoch Training Loss:  262.9601217270018  Validation Loss:  140.0998171637527\n",
      "passed  92 epoch Training Loss:  260.6364293412794  Validation Loss:  166.38687007199178\n",
      "passed  93 epoch Training Loss:  259.7799023648006  Validation Loss:  182.22897383156212\n",
      "passed  94 epoch Training Loss:  261.95039624675076  Validation Loss:  157.49300079990857\n",
      "passed  95 epoch Training Loss:  263.3050656184619  Validation Loss:  161.50018569306366\n",
      "passed  96 epoch Training Loss:  263.81827173017183  Validation Loss:  163.74072963089932\n",
      "passed  97 epoch Training Loss:  260.4854117796234  Validation Loss:  158.46731802079762\n",
      "passed  98 epoch Training Loss:  262.5861155138528  Validation Loss:  156.00691349560051\n",
      "passed  99 epoch Training Loss:  259.1750713244151  Validation Loss:  157.58311907210606\n",
      "passed  100 epoch Training Loss:  259.06062258289484  Validation Loss:  175.27848245914754\n",
      "passed  101 epoch Training Loss:  258.14049324795536  Validation Loss:  160.99587190035425\n",
      "passed  102 epoch Training Loss:  259.34503265073226  Validation Loss:  139.24692892240887\n",
      "passed  103 epoch Training Loss:  260.0084701705446  Validation Loss:  149.1623814421209\n",
      "passed  104 epoch Training Loss:  258.9164775248843  Validation Loss:  162.11790081133586\n",
      "passed  105 epoch Training Loss:  260.64837380333483  Validation Loss:  148.60200262827107\n",
      "passed  106 epoch Training Loss:  260.49986686109173  Validation Loss:  185.27993943549308\n",
      "passed  107 epoch Training Loss:  259.5813605528435  Validation Loss:  157.4675322820249\n",
      "passed  108 epoch Training Loss:  256.9543396944145  Validation Loss:  144.58380470803337\n",
      "passed  109 epoch Training Loss:  256.5051416978381  Validation Loss:  161.97954519483488\n",
      "passed  110 epoch Training Loss:  256.49555569644326  Validation Loss:  160.5830762198606\n",
      "passed  111 epoch Training Loss:  260.1443225765549  Validation Loss:  168.5022283167638\n",
      "passed  112 epoch Training Loss:  257.63380460280223  Validation Loss:  174.55365101131298\n",
      "passed  113 epoch Training Loss:  257.3634184999683  Validation Loss:  154.44846303279624\n",
      "passed  114 epoch Training Loss:  259.54770810879353  Validation Loss:  165.3015655353674\n",
      "passed  115 epoch Training Loss:  255.0278070119825  Validation Loss:  168.20984744600617\n",
      "passed  116 epoch Training Loss:  261.12283015279274  Validation Loss:  171.089918295052\n",
      "passed  117 epoch Training Loss:  258.22314081024535  Validation Loss:  152.27753970974746\n",
      "passed  118 epoch Training Loss:  255.97334685855577  Validation Loss:  178.79573762998515\n",
      "passed  119 epoch Training Loss:  256.9009826919419  Validation Loss:  159.9268655010856\n",
      "passed  120 epoch Training Loss:  258.63153490141383  Validation Loss:  198.2689549765741\n",
      "passed  121 epoch Training Loss:  261.90644772712864  Validation Loss:  189.319849160096\n",
      "passed  122 epoch Training Loss:  255.7723451467698  Validation Loss:  171.53695291966633\n",
      "passed  123 epoch Training Loss:  256.0717682115007  Validation Loss:  173.46147583133356\n",
      "passed  124 epoch Training Loss:  263.61858872757244  Validation Loss:  172.8753713861273\n",
      "passed  125 epoch Training Loss:  258.2961009319724  Validation Loss:  162.801279853731\n",
      "passed  126 epoch Training Loss:  259.30608001014394  Validation Loss:  191.52895383384757\n",
      "passed  127 epoch Training Loss:  258.69285487858997  Validation Loss:  169.92005199405781\n",
      "passed  128 epoch Training Loss:  257.031103784949  Validation Loss:  149.74460061707234\n",
      "passed  129 epoch Training Loss:  256.9599442084575  Validation Loss:  150.7714832590561\n",
      "passed  130 epoch Training Loss:  256.04889367907185  Validation Loss:  160.97617415152553\n",
      "passed  131 epoch Training Loss:  256.4590375958917  Validation Loss:  171.9212375728488\n",
      "passed  132 epoch Training Loss:  254.9693780511  Validation Loss:  171.54766598103075\n",
      "passed  133 epoch Training Loss:  255.3240347429151  Validation Loss:  148.25062849960005\n",
      "passed  134 epoch Training Loss:  256.4327648513282  Validation Loss:  180.97056050737058\n",
      "passed  135 epoch Training Loss:  255.97753122424396  Validation Loss:  176.52983944692036\n",
      "passed  136 epoch Training Loss:  255.27988334495657  Validation Loss:  161.29080962175752\n",
      "passed  137 epoch Training Loss:  253.13528181068915  Validation Loss:  177.64594046394697\n",
      "passed  138 epoch Training Loss:  254.38792873898433  Validation Loss:  177.60831619243515\n",
      "passed  139 epoch Training Loss:  255.4518988144297  Validation Loss:  156.14025539938294\n",
      "passed  140 epoch Training Loss:  258.50554745451086  Validation Loss:  177.61003028225346\n",
      "passed  141 epoch Training Loss:  255.2565776960629  Validation Loss:  175.94802022625984\n",
      "passed  142 epoch Training Loss:  253.45179737526152  Validation Loss:  181.30695063421322\n",
      "passed  143 epoch Training Loss:  253.72667216128826  Validation Loss:  187.76945491943778\n",
      "passed  144 epoch Training Loss:  252.40717682115007  Validation Loss:  172.95869043537883\n",
      "passed  145 epoch Training Loss:  254.93318962784505  Validation Loss:  174.45771911781512\n",
      "passed  146 epoch Training Loss:  256.1799023648006  Validation Loss:  176.6916638098503\n",
      "passed  147 epoch Training Loss:  254.2485766816712  Validation Loss:  159.40593932122044\n",
      "passed  148 epoch Training Loss:  252.580105243137  Validation Loss:  157.14701177008342\n",
      "passed  149 epoch Training Loss:  252.8763836936537  Validation Loss:  171.93270769054965\n",
      "passed  150 epoch Training Loss:  255.64098142395233  Validation Loss:  169.6600102845389\n",
      "passed  151 epoch Training Loss:  251.85582958219743  Validation Loss:  180.1895069134956\n",
      "passed  152 epoch Training Loss:  252.97445000950992  Validation Loss:  177.756242143755\n",
      "passed  153 epoch Training Loss:  254.24817092499842  Validation Loss:  189.68699291509543\n",
      "passed  154 epoch Training Loss:  251.24176757750587  Validation Loss:  170.49214375499943\n",
      "passed  155 epoch Training Loss:  255.11697204082927  Validation Loss:  163.92645126271285\n",
      "passed  156 epoch Training Loss:  251.76217587015788  Validation Loss:  154.67140898183064\n",
      "passed  157 epoch Training Loss:  253.88562733785582  Validation Loss:  191.71444692035195\n",
      "passed  158 epoch Training Loss:  253.599492804159  Validation Loss:  165.21506113587017\n",
      "passed  159 epoch Training Loss:  255.36476256894693  Validation Loss:  164.3465318249343\n",
      "passed  160 epoch Training Loss:  253.71287643441323  Validation Loss:  209.7719260655925\n",
      "passed  161 epoch Training Loss:  252.92309643060926  Validation Loss:  170.3981830647926\n",
      "passed  162 epoch Training Loss:  258.70890762695745  Validation Loss:  171.558550451377\n",
      "passed  163 epoch Training Loss:  255.4485766816712  Validation Loss:  175.12211461547253\n",
      "passed  164 epoch Training Loss:  251.48030178152538  Validation Loss:  179.45694777739686\n",
      "passed  165 epoch Training Loss:  250.9354720091295  Validation Loss:  181.302779682322\n",
      "passed  166 epoch Training Loss:  250.80497051924175  Validation Loss:  179.94930579362358\n",
      "passed  167 epoch Training Loss:  253.26869967666266  Validation Loss:  167.70706205005143\n",
      "passed  168 epoch Training Loss:  251.35144867812085  Validation Loss:  145.6108873271626\n",
      "passed  169 epoch Training Loss:  252.24829772395867  Validation Loss:  169.2366300994172\n",
      "passed  170 epoch Training Loss:  252.31098712990553  Validation Loss:  174.84577476859786\n",
      "passed  171 epoch Training Loss:  252.00471692132123  Validation Loss:  197.38241343846417\n",
      "passed  172 epoch Training Loss:  250.45363596018512  Validation Loss:  177.43812135755914\n",
      "passed  173 epoch Training Loss:  250.9302478919673  Validation Loss:  182.0100417095189\n",
      "passed  174 epoch Training Loss:  252.12875166423635  Validation Loss:  176.1941206719232\n",
      "passed  175 epoch Training Loss:  255.11486717808913  Validation Loss:  184.20299108673294\n",
      "passed  176 epoch Training Loss:  257.01332657072214  Validation Loss:  163.68930693635014\n",
      "passed  177 epoch Training Loss:  253.46402079502948  Validation Loss:  183.38531310707347\n",
      "passed  178 epoch Training Loss:  251.73575096684206  Validation Loss:  177.95773340189692\n",
      "passed  179 epoch Training Loss:  252.09824383440056  Validation Loss:  165.0904610901611\n",
      "passed  180 epoch Training Loss:  249.43726621441704  Validation Loss:  197.6009313221346\n",
      "passed  181 epoch Training Loss:  250.20825461231217  Validation Loss:  198.43692149468632\n",
      "passed  182 epoch Training Loss:  250.81420148354783  Validation Loss:  194.09657467717975\n",
      "passed  183 epoch Training Loss:  249.52148608381412  Validation Loss:  201.0665923894412\n",
      "passed  184 epoch Training Loss:  248.09223356368477  Validation Loss:  176.97044623471604\n",
      "passed  185 epoch Training Loss:  252.02049071197615  Validation Loss:  201.58670437664267\n",
      "passed  186 epoch Training Loss:  249.82537247194574  Validation Loss:  151.60610215975316\n",
      "passed  187 epoch Training Loss:  250.36041336461042  Validation Loss:  172.63775568506458\n",
      "passed  188 epoch Training Loss:  251.50160400684715  Validation Loss:  196.36841503828134\n",
      "passed  189 epoch Training Loss:  250.8159766689913  Validation Loss:  181.68102216889497\n",
      "passed  190 epoch Training Loss:  249.77516008368733  Validation Loss:  187.90282539138383\n",
      "passed  191 epoch Training Loss:  250.5759208774488  Validation Loss:  205.41738087075763\n",
      "passed  192 epoch Training Loss:  248.99458568439738  Validation Loss:  181.59567478002515\n",
      "passed  193 epoch Training Loss:  249.07680213022255  Validation Loss:  181.4698320191978\n",
      "passed  194 epoch Training Loss:  248.82784505167058  Validation Loss:  191.99672894526338\n",
      "passed  195 epoch Training Loss:  249.89085145501807  Validation Loss:  182.76588389898296\n",
      "passed  196 epoch Training Loss:  251.30252963925696  Validation Loss:  178.72410295966176\n",
      "passed  197 epoch Training Loss:  250.29475686299372  Validation Loss:  173.52603988115644\n",
      "passed  198 epoch Training Loss:  252.38430228872124  Validation Loss:  206.14686892926522\n",
      "passed  199 epoch Training Loss:  251.69345083370317  Validation Loss:  205.89264084104673\n",
      "passed  200 epoch Training Loss:  251.3761744753693  Validation Loss:  192.24870014855446\n",
      "passed  201 epoch Training Loss:  250.38379509288023  Validation Loss:  188.58304765169694\n",
      "passed  202 epoch Training Loss:  252.9134597096304  Validation Loss:  191.7659838875557\n",
      "passed  203 epoch Training Loss:  248.75909465542384  Validation Loss:  184.46660381670665\n",
      "passed  204 epoch Training Loss:  248.54422113738667  Validation Loss:  185.80345103416752\n",
      "passed  205 epoch Training Loss:  248.3615799150447  Validation Loss:  172.69610615929608\n",
      "passed  206 epoch Training Loss:  248.740810245356  Validation Loss:  180.0388098503028\n",
      "passed  207 epoch Training Loss:  250.01257845685666  Validation Loss:  182.3145926179865\n",
      "passed  208 epoch Training Loss:  248.77562923984024  Validation Loss:  172.45427665409667\n",
      "passed  209 epoch Training Loss:  248.38207062702085  Validation Loss:  166.59263227059765\n",
      "passed  210 epoch Training Loss:  248.56308882267166  Validation Loss:  189.43350759913153\n",
      "passed  211 epoch Training Loss:  247.3277119127623  Validation Loss:  199.7231744943435\n",
      "passed  212 epoch Training Loss:  248.14624992075065  Validation Loss:  186.21876071306136\n",
      "passed  213 epoch Training Loss:  246.68266024218602  Validation Loss:  180.31820649068678\n",
      "passed  214 epoch Training Loss:  248.89314651619856  Validation Loss:  194.10818763569878\n",
      "passed  215 epoch Training Loss:  248.21109490902174  Validation Loss:  192.54223802993943\n",
      "passed  216 epoch Training Loss:  247.67062702085843  Validation Loss:  191.41766655239402\n",
      "passed  217 epoch Training Loss:  249.11529829455398  Validation Loss:  173.91166723802993\n",
      "passed  218 epoch Training Loss:  247.19589171368796  Validation Loss:  167.22823105930752\n",
      "passed  219 epoch Training Loss:  247.31069549229696  Validation Loss:  176.00507084904584\n",
      "passed  220 epoch Training Loss:  247.23659417992772  Validation Loss:  175.7634413209919\n",
      "passed  221 epoch Training Loss:  246.61018195650794  Validation Loss:  176.75307107759113\n",
      "passed  222 epoch Training Loss:  246.43228301527927  Validation Loss:  165.2456719232088\n",
      "passed  223 epoch Training Loss:  248.7783173777975  Validation Loss:  191.9519340646783\n",
      "passed  224 epoch Training Loss:  249.63819184682686  Validation Loss:  193.2727259741744\n",
      "passed  225 epoch Training Loss:  248.6024472199328  Validation Loss:  182.1577676836933\n",
      "passed  226 epoch Training Loss:  246.37291574209092  Validation Loss:  187.58577591132442\n",
      "passed  227 epoch Training Loss:  247.24544474735308  Validation Loss:  195.81453548165925\n",
      "passed  228 epoch Training Loss:  247.19774297850756  Validation Loss:  211.18503599588618\n",
      "passed  229 epoch Training Loss:  247.94045520826728  Validation Loss:  190.79268083647582\n",
      "passed  230 epoch Training Loss:  251.35976668991313  Validation Loss:  203.23934407496287\n",
      "passed  231 epoch Training Loss:  247.4970899638623  Validation Loss:  187.83444749171522\n",
      "passed  232 epoch Training Loss:  246.80808977366385  Validation Loss:  188.67657981944922\n",
      "passed  233 epoch Training Loss:  248.5084131110125  Validation Loss:  216.87712832819108\n",
      "passed  234 epoch Training Loss:  246.67053826158624  Validation Loss:  174.09398925837047\n",
      "passed  235 epoch Training Loss:  246.05708489190388  Validation Loss:  196.57584847446006\n",
      "passed  236 epoch Training Loss:  246.38181702910038  Validation Loss:  208.564792595132\n",
      "passed  237 epoch Training Loss:  246.06522538515185  Validation Loss:  178.18330762198605\n",
      "passed  238 epoch Training Loss:  245.42943003867367  Validation Loss:  187.8146497543138\n",
      "passed  239 epoch Training Loss:  245.13013377290306  Validation Loss:  187.4154239515484\n",
      "passed  240 epoch Training Loss:  246.12740759525772  Validation Loss:  211.2693834990287\n",
      "passed  241 epoch Training Loss:  245.89932162556266  Validation Loss:  170.8808707576277\n",
      "passed  242 epoch Training Loss:  244.25248208964686  Validation Loss:  183.42759398925838\n",
      "passed  243 epoch Training Loss:  246.7172509985418  Validation Loss:  192.4904868015084\n",
      "passed  244 epoch Training Loss:  246.9819311481646  Validation Loss:  181.89668323620157\n",
      "passed  245 epoch Training Loss:  245.87743612502376  Validation Loss:  190.27286881499256\n",
      "passed  246 epoch Training Loss:  245.99647498890508  Validation Loss:  182.62394297794538\n",
      "passed  247 epoch Training Loss:  244.34950865402902  Validation Loss:  190.02138327048337\n",
      "passed  248 epoch Training Loss:  244.62788309135865  Validation Loss:  205.8915695349103\n",
      "passed  249 epoch Training Loss:  243.26679769225893  Validation Loss:  189.05940749628613\n",
      "passed  250 epoch Training Loss:  244.34212895454257  Validation Loss:  185.65968175065706\n",
      "passed  251 epoch Training Loss:  245.40054523552908  Validation Loss:  194.36635813049938\n",
      "passed  252 epoch Training Loss:  245.16350725924048  Validation Loss:  198.39644040681065\n"
     ]
    }
   ],
   "source": [
    "params1 = [242, 242, 3]\n",
    "params2 = [72, 180, 106]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\");\n",
    "best_val_loss = Train_and_Evaluate2(TrainingLoader, ValidationLoader, device, params1, params2, 2000, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "q-JWl1d3GyOP",
    "outputId": "e8ee81b7-b2f8-4bc9-b872-ddfd6bc44846"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Testing:  45351.613\n",
      "MAE for Testing:  150.6994\n",
      "MAPE for Testing:  tensor(0.0522)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from torchmetrics import MeanAbsolutePercentageError\n",
    "\n",
    "model = torch.load(\"/home/jik19004/FilesToRun/BaselineLSTM20/BaselineLSTM20(BEST)\")\n",
    "predictions = predict(model, TestingLoader, device = torch.device(\"cuda\"))\n",
    "MAE_result = mean_absolute_error(predictions[0], predictions[1])\n",
    "MSE_result = mean_squared_error(predictions[0], predictions[1])\n",
    "MAPE = MeanAbsolutePercentageError() \n",
    "MAPE_result = MAPE(torch.Tensor(predictions[0]), torch.Tensor(predictions[1]))\n",
    "print(\"MSE for Testing: \", MSE_result)\n",
    "print(\"MAE for Testing: \", MAE_result)\n",
    "print(\"MAPE for Testing: \", MAPE_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for Validation:  125.12739\n",
      "MAPE for Validation:  tensor(0.0395)\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(model, ValidationLoader, device = torch.device(\"cuda\"))\n",
    "MAE_result = mean_absolute_error(predictions[0], predictions[1])\n",
    "MAPE = MeanAbsolutePercentageError()\n",
    "MAPE_result = MAPE(torch.Tensor(predictions[0]), torch.Tensor(predictions[1]))\n",
    "\n",
    "print(\"MAE for Validation: \", MAE_result)\n",
    "print(\"MAPE for Validation: \", MAPE_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for Training:  286.58823\n",
      "MAPE for Training:  tensor(0.0800)\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(model, TrainingLoader, device = torch.device(\"cuda\"))\n",
    "MAE_result = mean_absolute_error(predictions[0], predictions[1])\n",
    "MAPE = MeanAbsolutePercentageError()\n",
    "MAPE_result = MAPE(torch.Tensor(predictions[0]), torch.Tensor(predictions[1]))\n",
    "\n",
    "\n",
    "print(\"MAE for Training: \", MAE_result)\n",
    "print(\"MAPE for Training: \", MAPE_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "TrainEpochLoss = best_val_loss[0]\n",
    "ValidationEpochLoss = best_val_loss[1]\n",
    "fig = plt.figure()\n",
    "axes = fig.add_axes([0,0,1.7,1.25])\n",
    "axes.plot(TrainEpochLoss, label = \"Training Loss\")\n",
    "axes.plot(ValidationEpochLoss, label = \"Validation Loss\")\n",
    "axes.set_yticks(np.arange(0, 3000, 200))\n",
    "axes.set_title(\"Training and Validation Loss for Bayesian LSTM\")\n",
    "axes.set_xlabel(\"Epochs\")\n",
    "axes.set_ylabel(\"Loss: Mean Absolute Error\")\n",
    "axes.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUS459C6ro22"
   },
   "source": [
    "# Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGYj2vTl311y"
   },
   "source": [
    "The Bayesian LSTM implemented is shown to produce reasonably accurate and sensible results on both the training and test sets, often comparable to other existing frequentist machine learning and deep learning methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv('/home/jik19004/ASOS_10_CT_stations_tmpc_demand_2011_2023.csv').drop(columns = [\"Unnamed: 0\"])\n",
    "x.ffill(inplace = True)\n",
    "x.bfill(inplace = True) # fill in missing values with the previous value.\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iERCnyhqGyOP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime \n",
    "x = pd.read_csv(\"/home/jik19004/ASOS_10_CT_stations_tmpc_demand_2011_2023.csv\")[\"Datetime\"]\n",
    "dateList = []\n",
    "for i in range(len(x)): ## just create the datetimes so that we can use it for graph plotting.\n",
    "    dateList.append(datetime.strptime(x[i], \"%m/%d/%Y %H:%M\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "efoE_qxFGyOP",
    "outputId": "812668ef-98f3-466b-e21c-af213b1bf00f"
   },
   "outputs": [],
   "source": [
    "x = np.array(pd.read_csv(\"/home/jik19004/ASOS_10_CT_stations_tmpc_demand_2011_2023.csv\")[\"Demand\"])\n",
    "x = pd.DataFrame({\"DateTime\": dateList, \"Demand\": x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07qm7YlwGyOP"
   },
   "outputs": [],
   "source": [
    "training_df = x.iloc[:78883, :]\n",
    "validation_df = x.iloc[87667:105187, :]\n",
    "testing_df = x.iloc[105187:, :] #splits for training, validation, and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jrwiy646yq7t"
   },
   "source": [
    "# Uncertainty Quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FEzMrU147zx"
   },
   "source": [
    "The fact that stochastic dropouts are applied after each LSTM layer in the Bayesian LSTM enables users to interpret the model outputs as random samples from the posterior distribution of the target variable.\n",
    "\n",
    "This implies that by running multiple experiments/predictions, can approximate  parameters of the posterioir distribution, namely the mean and the variance, in order to create confidence intervals for each prediction.\n",
    "\n",
    "In this example, we construct 99% confidence intervals that are three standard deviations away from the approximate mean of each prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PT7jTMoqGyOQ",
    "outputId": "89a6ddd3-33df-4804-dbec-bf511e3ec38f"
   },
   "outputs": [],
   "source": [
    "bayesian_lstm = torch.load(\"/home/jik19004/FilesToRun/BayesianLSTM20/BayesianLSTM20BEST(NOW)\")\n",
    "print(bayesian_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3B5Vbkn-GyOQ"
   },
   "outputs": [],
   "source": [
    "TrainingPredictions, _ = predict(bayesian_lstm, TrainingLoader, torch.device(\"cuda\"))\n",
    "ValidationPredictions, _ = predict(bayesian_lstm, ValidationLoader, torch.device(\"cuda\"))\n",
    "TestingPredictions, _ = predict(bayesian_lstm, TestingLoader, torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBJDyf2fGyOQ"
   },
   "outputs": [],
   "source": [
    "training_df = pd.DataFrame()\n",
    "validation_df = pd.DataFrame()\n",
    "testing_df = pd.DataFrame()\n",
    "\n",
    "training_df[\"Date\"] = x.iloc[18:78883, 0]\n",
    "training_df[\"Actual Train Output\"] = x.iloc[18:78883, 1]\n",
    "training_df[\"Predicted Train Output\"] = TrainingPredictions\n",
    "\n",
    "validation_df[\"Date\"] = x.iloc[87667+18:105187,0]\n",
    "validation_df[\"Actual Val Output\"] = x.iloc[87667+18:105187, 1]\n",
    "validation_df[\"Predicted Val Output\"] = ValidationPredictions\n",
    "\n",
    "testing_df[\"Date\"] = x.iloc[105187 + 18 :, 0]\n",
    "testing_df[\"Actual Test Output\"] = x.iloc[105187 + 18:, 1]\n",
    "testing_df[\"Predicted Test Output\"] = TestingPredictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jb4xyVW6DVUV",
    "outputId": "110ad77c-1090-43bb-83c0-15a099209a13"
   },
   "outputs": [],
   "source": [
    "n_experiments = 500\n",
    "\n",
    "\n",
    "test_uncertainty_df = pd.DataFrame()\n",
    "test_uncertainty_df['Date'] = testing_df['Date']\n",
    "\n",
    "\n",
    "for i in range(n_experiments):\n",
    "  experiment_predictions, _ = predict(bayesian_lstm, TestingLoader, torch.device(\"cuda\"))\n",
    "  test_uncertainty_df['energy_demand_{}'.format(i)] = experiment_predictions\n",
    "\n",
    "\n",
    "energy_consumption_df = test_uncertainty_df.filter(like='energy_demand', axis=1)\n",
    "test_uncertainty_df2 = test_uncertainty_df.copy() # copy and creat a duplicate of the dataframe. \n",
    "\n",
    "test_uncertainty_df['energy_demand_mean'] = energy_consumption_df.mean(axis=1)\n",
    "test_uncertainty_df['energy_demand_std'] = energy_consumption_df.std(axis=1)\n",
    "\n",
    "test_uncertainty_df = test_uncertainty_df[['Date', 'energy_demand_mean', 'energy_demand_std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_uncertainty_df2.to_csv(\"Testing500.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHRALjusGyOR"
   },
   "outputs": [],
   "source": [
    "arr = np.array(test_uncertainty_df['energy_demand_mean'])\n",
    "actualVals = np.array(testing_df[\"Actual Test Output\"])\n",
    "\n",
    "error = mean_absolute_error(arr, actualVals)\n",
    "mape_error = MAPE(torch.Tensor(arr), torch.Tensor(actualVals))\n",
    "\n",
    "print(error)\n",
    "print(mape_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_experiments = 500\n",
    "\n",
    "\n",
    "validation_uncertainty_df = pd.DataFrame()\n",
    "validation_uncertainty_df['Date'] = validation_df['Date']\n",
    "\n",
    "\n",
    "for i in range(n_experiments):\n",
    "  experiment_predictions, _ = predict(bayesian_lstm, ValidationLoader, torch.device(\"cuda\"))\n",
    "  validation_uncertainty_df['energy_demand_{}'.format(i)] = experiment_predictions\n",
    "\n",
    "\n",
    "energy_consumption_df = validation_uncertainty_df.filter(like='energy_demand', axis=1)\n",
    "validation_uncertainty_df2 = validation_uncertainty_df.copy() # copy and creat a duplicate of the dataframe. \n",
    "\n",
    "validation_uncertainty_df['energy_demand_mean'] = energy_consumption_df.mean(axis=1)\n",
    "validation_uncertainty_df['energy_demand_std'] = energy_consumption_df.std(axis=1)\n",
    "\n",
    "validation_uncertainty_df = validation_uncertainty_df[['Date', 'energy_demand_mean', 'energy_demand_std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_uncertainty_df2.to_csv(\"Validation500.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(validation_uncertainty_df['energy_demand_mean'])\n",
    "actualVals = np.array(validation_df[\"Actual Val Output\"])\n",
    "\n",
    "error = mean_absolute_error(arr, actualVals)\n",
    "mape_error = MAPE(torch.Tensor(arr), torch.Tensor(actualVals))\n",
    "\n",
    "print(error)\n",
    "print(mape_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_experiments = 500\n",
    "\n",
    "\n",
    "training_uncertainty_df = pd.DataFrame()\n",
    "training_uncertainty_df['Date'] = training_df['Date']\n",
    "\n",
    "\n",
    "for i in range(n_experiments):\n",
    "  experiment_predictions, _ = predict(bayesian_lstm, TrainingLoader, torch.device(\"cuda\"))\n",
    "  training_uncertainty_df['energy_demand_{}'.format(i)] = experiment_predictions\n",
    "\n",
    "\n",
    "energy_consumption_df = training_uncertainty_df.filter(like='energy_demand', axis=1)\n",
    "training_uncertainty_df2 = training_uncertainty_df.copy() # copy and creat a duplicate of the dataframe. \n",
    "\n",
    "training_uncertainty_df['energy_demand_mean'] = energy_consumption_df.mean(axis=1)\n",
    "training_uncertainty_df['energy_demand_std'] = energy_consumption_df.std(axis=1)\n",
    "\n",
    "training_uncertainty_df = training_uncertainty_df[['Date', 'energy_demand_mean', 'energy_demand_std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_uncertainty_df2.to_csv(\"Training500.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array(training_uncertainty_df['energy_demand_mean'])\n",
    "actualVals = np.array(training_df[\"Actual Train Output\"])\n",
    "\n",
    "error = mean_absolute_error(arr, actualVals)\n",
    "mape_error = MAPE(torch.Tensor(arr), torch.Tensor(actualVals))\n",
    "\n",
    "print(error)\n",
    "print(mape_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNrb70dSdDH0"
   },
   "outputs": [],
   "source": [
    "test_uncertainty_df['99% lower_bound'] = test_uncertainty_df['energy_demand_mean'] - 3*test_uncertainty_df['energy_demand_std'] #99 % confidence interval\n",
    "test_uncertainty_df['99% upper_bound'] = test_uncertainty_df['energy_demand_mean'] + 3*test_uncertainty_df['energy_demand_std'] #99 % confidene interval\n",
    "\n",
    "test_uncertainty_df[\"99.99% lower_bound\"] = test_uncertainty_df[\"energy_demand_mean\"] - 3.15 * test_uncertainty_df[\"energy_demand_std\"]\n",
    "test_uncertainty_df[\"99.99% upper_bound\"] = test_uncertainty_df[\"energy_demand_mean\"] + 3.15 * test_uncertainty_df[\"energy_demand_std\"]\n",
    "\n",
    "test_uncertainty_df[\"99.999% lower_bound\"] = test_uncertainty_df[\"energy_demand_mean\"] - 3.3 * test_uncertainty_df[\"energy_demand_std\"]\n",
    "test_uncertainty_df[\"99.999% upper_bound\"] = test_uncertainty_df[\"energy_demand_mean\"] + 3.3 * test_uncertainty_df[\"energy_demand_std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "WdHylS8OdEHt",
    "outputId": "a366f060-d302-40e9-d065-19cdefcbddfb"
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "test_uncertainty_plot_df = test_uncertainty_df.copy(deep=True)\n",
    "test_uncertainty_plot_df = test_uncertainty_plot_df.loc[test_uncertainty_plot_df['Date'].between('2022-01-04 06:00:00', '2022-1-06 23:00:00')]\n",
    "truth_uncertainty_plot_df = testing_df.copy(deep=True)\n",
    "truth_uncertainty_plot_df = truth_uncertainty_plot_df.loc[testing_df['Date'].between('2022-01-04 06:00:00', '2022-1-06 23:00:00')]\n",
    "\n",
    "upper_trace = go.Scatter(\n",
    "    x=test_uncertainty_plot_df['Date'],\n",
    "    y=test_uncertainty_plot_df['99% upper_bound'],\n",
    "    mode='lines',\n",
    "    fill=None,\n",
    "    name='99 Upper Confidence Bound'\n",
    "    )\n",
    "lower_trace = go.Scatter(\n",
    "    x=test_uncertainty_plot_df['Date'],\n",
    "    y=test_uncertainty_plot_df['99% lower_bound'],\n",
    "    mode='lines',\n",
    "    fill='tonexty',\n",
    "    fillcolor='rgba(255, 211, 0, 0.1)',\n",
    "    name='99 lower Confidence Bound'\n",
    "    )\n",
    "real_trace = go.Scatter(\n",
    "    x=truth_uncertainty_plot_df['Date'],\n",
    "    y=truth_uncertainty_plot_df['Actual Test Output'],\n",
    "    mode='lines',\n",
    "    fill=None,\n",
    "    name='Real Values'\n",
    "    )\n",
    "\n",
    "data = [upper_trace, lower_trace, real_trace]\n",
    "\n",
    "fig = go.Figure(data=data)\n",
    "fig.update_layout(title='Uncertainty Quantification for Energy Consumption Test Data',\n",
    "                   xaxis_title='Time',\n",
    "                   yaxis_title='Energy Demand (kWh)')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7THEK4P96J0S"
   },
   "source": [
    "#### Evaluating Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPuR8L6D6PkL"
   },
   "source": [
    "Using multiple experiments above, 99% confidence intervals have been constructed for each the prediction of the target variable (the logarithm of appliance power consumption). While we can visually observe that the model is generally capturing the behavior of the time-series, approximately only 50% of the real data points lie within a 99% confidence interval from the mean prediction value.\n",
    "\n",
    "Despite the relatively low percentage of points within the confidence interval, it must be noted that Bayesian Neural Networks only seek to quantify the epistemic model uncertainty and does not account for aleatoric uncertainty (i.e. noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mV2_6qekxzLn",
    "outputId": "31c0cd39-7eb6-4b7d-9f75-f139678ab76f"
   },
   "outputs": [],
   "source": [
    "bounds_df = pd.DataFrame()\n",
    "\n",
    "# Using 99% confidence bounds\n",
    "\n",
    "bounds_df['lower_bound'] = test_uncertainty_df['99.999% lower_bound']\n",
    "bounds_df['prediction'] = test_uncertainty_df['energy_demand_mean']\n",
    "bounds_df['real_value'] = testing_df[\"Actual Test Output\"]\n",
    "bounds_df['upper_bound'] = test_uncertainty_df['99.999% upper_bound']\n",
    "\n",
    "bounds_df['contained'] = ((bounds_df['real_value'] >= bounds_df['lower_bound']) &\n",
    "                          (bounds_df['real_value'] <= bounds_df['upper_bound']))\n",
    "\n",
    "print(\"Proportion of points for testing contained within 99.999% confidence interval:\",\n",
    "      bounds_df['contained'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bounds_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_uncertainty_df['99% lower_bound'] = validation_uncertainty_df['energy_demand_mean'] - 3*validation_uncertainty_df['energy_demand_std'] #99 % confidence interval\n",
    "validation_uncertainty_df['99% upper_bound'] = validation_uncertainty_df['energy_demand_mean'] + 3*validation_uncertainty_df['energy_demand_std'] #99 % confidene interval\n",
    "\n",
    "validation_uncertainty_df['99.99% lower_bound'] = validation_uncertainty_df['energy_demand_mean'] - 3.15*validation_uncertainty_df['energy_demand_std'] #99 % confidence interval\n",
    "validation_uncertainty_df['99.99% upper_bound'] = validation_uncertainty_df['energy_demand_mean'] + 3.15*validation_uncertainty_df['energy_demand_std'] #99 % confidene interval\n",
    "\n",
    "validation_uncertainty_df['99.999% lower_bound'] = validation_uncertainty_df['energy_demand_mean'] - 3*validation_uncertainty_df['energy_demand_std'] #99 % confidence interval\n",
    "validation_uncertainty_df['99.999% upper_bound'] = validation_uncertainty_df['energy_demand_mean'] + 3*validation_uncertainty_df['energy_demand_std'] #99 % confidene interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_df = pd.DataFrame()\n",
    "\n",
    "# Using 99% confidence bounds\n",
    "bounds_df['lower_bound'] = validation_uncertainty_df['99% lower_bound']\n",
    "bounds_df['prediction'] = validation_uncertainty_df['energy_demand_mean']\n",
    "bounds_df['real_value'] = validation_df['Actual Val Output']\n",
    "bounds_df['upper_bound'] = validation_uncertainty_df['99% upper_bound']\n",
    "\n",
    "bounds_df['contained'] = ((bounds_df['real_value'] >= bounds_df['lower_bound']) &\n",
    "                          (bounds_df['real_value'] <= bounds_df['upper_bound']))\n",
    "\n",
    "print(\"Proportion of points for validation contained within 99% confidence interval:\",\n",
    "      bounds_df['contained'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_uncertainty_df['99% lower_bound'] = training_uncertainty_df['energy_demand_mean'] - 3*training_uncertainty_df['energy_demand_std'] #99 % confidence interval\n",
    "training_uncertainty_df[\"99% upper_bound\"] = training_uncertainty_df['energy_demand_mean'] + 3*training_uncertainty_df['energy_demand_std'] #99 % confidene interval\n",
    "\n",
    "training_uncertainty_df['99.99% lower_bound'] = training_uncertainty_df['energy_demand_mean'] - 3.15*training_uncertainty_df['energy_demand_std'] #99 % confidence interval\n",
    "training_uncertainty_df[\"99.99% upper_bound\"] = training_uncertainty_df['energy_demand_mean'] + 3.15*training_uncertainty_df['energy_demand_std'] #99 % confidene interval\n",
    "\n",
    "training_uncertainty_df['99.999% lower_bound'] = training_uncertainty_df['energy_demand_mean'] - 3.3*training_uncertainty_df['energy_demand_std'] #99 % confidence interval\n",
    "training_uncertainty_df[\"99.999% upper_bound\"] = training_uncertainty_df['energy_demand_mean'] + 3.3*training_uncertainty_df['energy_demand_std'] #99 % confidene interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_df = pd.DataFrame()\n",
    "\n",
    "bounds_df['lower_bound'] = training_uncertainty_df['99.999% lower_bound']\n",
    "bounds_df['prediction'] = training_uncertainty_df['energy_demand_mean']\n",
    "bounds_df['real_value'] = training_df['Actual Train Output']\n",
    "bounds_df['upper_bound'] = training_uncertainty_df['99.999% upper_bound']\n",
    "\n",
    "bounds_df['contained'] = ((bounds_df['real_value'] >= bounds_df['lower_bound']) &\n",
    "                            (bounds_df['real_value'] <= bounds_df['upper_bound']))\n",
    "\n",
    "print(\"Proportion of points for training contained within 99.999% confidence interval:\",\n",
    "        bounds_df['contained'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df[\"Actual Train Output\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(training_uncertainty_df[\"energy_demand_mean\"].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_uncertainty_df = training_uncertainty_df.copy()\n",
    "training_uncertainty_df.reset_index(drop = True, inplace = True)\n",
    "Errors = []\n",
    "\n",
    "index = 0 \n",
    "for prediction in training_uncertainty_df[\"energy_demand_mean\"]:\n",
    "    Errors.append(mean_absolute_error([prediction], [np.float32(training_df[\"Actual Train Output\"].iloc[index])]))\n",
    "    index+=1 \n",
    "\n",
    "TrainingErrors = pd.DataFrame({\"Date\": training_uncertainty_df[\"Date\"], \"Errors\": Errors})\n",
    "print(f\"The average training error: {np.mean(Errors)}\")\n",
    "print(f\"The standard deviation of the training error: \", np.std(Errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MonthErrors = {1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[], 10:[], 11:[], 12:[]}\n",
    "\n",
    "for i in range(len(training_uncertainty_df[\"Date\"])):\n",
    "    date = training_uncertainty_df[\"Date\"].iloc[i]\n",
    "    MonthErrors[date.month].append(Errors[i])\n",
    "    \n",
    "for key in MonthErrors.keys():\n",
    "    mean = np.mean(MonthErrors[key])\n",
    "    std = np.std(MonthErrors[key])\n",
    "    MonthErrors[key] = (mean, std)\n",
    "    print(f\"Month {key}: Mean Error: {mean}, Standard deviation: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming MonthErrors is a dictionary where:\n",
    "# - keys are months\n",
    "# - values are lists where the first element is the mean error and the second element is the standard deviation\n",
    "\n",
    "# Extracting months, mean errors, and standard deviations\n",
    "months = list(MonthErrors.keys())\n",
    "mean_errors = [MonthErrors[key][0] for key in months]\n",
    "std_devs = [MonthErrors[key][1] for key in months]\n",
    "error_bars = [(0, std) for std in std_devs]\n",
    "\n",
    "# Creating the bar plot with error bars\n",
    "plt.bar(months, mean_errors, yerr=error_bars, capsize=5)  # capsize specifies the width of the horizontal line at the top of the error bar\n",
    "\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Error in MWH\")\n",
    "plt.title(\"Monthly Error with Standard Deviation for Training\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Errors = []\n",
    "\n",
    "index = 0 \n",
    "for prediction in validation_uncertainty_df[\"energy_demand_mean\"]:\n",
    "    Errors.append(mean_absolute_error([prediction], [np.float32(validation_df[\"Actual Val Output\"].iloc[index])]))\n",
    "    index+=1 \n",
    "\n",
    "TrainingErrors = pd.DataFrame({\"Date\": validation_uncertainty_df[\"Date\"], \"Errors\": Errors})\n",
    "print(f\"The average validation error: {np.mean(Errors)}\")\n",
    "print(f\"The standard deviation of the validation error: \", np.std(Errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MonthErrors = {1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[], 10:[], 11:[], 12:[]}\n",
    "\n",
    "for i in range(len(validation_uncertainty_df[\"Date\"])):\n",
    "    date = validation_uncertainty_df[\"Date\"].iloc[i]\n",
    "    MonthErrors[date.month].append(Errors[i])\n",
    "    \n",
    "for key in MonthErrors.keys():\n",
    "    mean = np.mean(MonthErrors[key])\n",
    "    std = np.std(MonthErrors[key])\n",
    "    MonthErrors[key] = (mean, std)\n",
    "    print(f\"Month {key}: Mean Error: {mean}, Standard deviation: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming MonthErrors is a dictionary where:\n",
    "# - keys are months\n",
    "# - values are lists where the first element is the mean error and the second element is the standard deviation\n",
    "\n",
    "# Extracting months, mean errors, and standard deviations\n",
    "months = list(MonthErrors.keys())\n",
    "mean_errors = [MonthErrors[key][0] for key in months]\n",
    "std_devs = [MonthErrors[key][1] for key in months]\n",
    "\n",
    "# Specifying that the lower part of the error bar is 0 and the upper part is the standard deviation\n",
    "error_bars = np.array([(0,)*len(std_devs), std_devs])\n",
    "\n",
    "# Creating the bar plot with error bars\n",
    "plt.bar(months, mean_errors, yerr=error_bars, capsize=5)\n",
    "\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Error in MWH\")\n",
    "plt.title(\"Monthly Error with Upper Standard Deviation\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_uncertainty_df.reset_index(inplace = True, drop = True)\n",
    "Errors = []\n",
    "index = 0 \n",
    "for prediction in test_uncertainty_df[\"energy_demand_mean\"]:\n",
    "    Errors.append(mean_absolute_error([prediction], [np.float32(testing_df[\"Actual Test Output\"].iloc[index])]))\n",
    "    index+=1 \n",
    "\n",
    "TrainingErrors = pd.DataFrame({\"Date\": test_uncertainty_df[\"Date\"], \"Errors\": Errors})\n",
    "print(f\"The average validation error: {np.mean(Errors)}\")\n",
    "print(f\"The standard deviation of the validation error: \", np.std(Errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MonthErrors = {1:[], 2:[], 3:[], 4:[], 5:[], 6:[], 7:[], 8:[], 9:[], 10:[], 11:[], 12:[]}\n",
    "\n",
    "for i in range(len(test_uncertainty_df[\"Date\"])):\n",
    "    date = test_uncertainty_df[\"Date\"].iloc[i]\n",
    "    MonthErrors[date.month].append(Errors[i])\n",
    "    \n",
    "for key in MonthErrors.keys():\n",
    "    mean = np.mean(MonthErrors[key])\n",
    "    std = np.std(MonthErrors[key])\n",
    "    MonthErrors[key] = (mean, std)\n",
    "    print(f\"Month {key}: Mean Error: {mean}, Standard deviation: {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming MonthErrors is a dictionary where:\n",
    "# - keys are months\n",
    "# - values are lists where the first element is the mean error and the second element is the standard deviation\n",
    "\n",
    "# Extracting months, mean errors, and standard deviations\n",
    "months = list(MonthErrors.keys())\n",
    "mean_errors = [MonthErrors[key][0] for key in months]\n",
    "std_devs = [MonthErrors[key][1] for key in months]\n",
    "\n",
    "# Specifying that the lower part of the error bar is 0 and the upper part is the standard deviation\n",
    "error_bars = np.array([(0,)*len(std_devs), std_devs])\n",
    "\n",
    "# Creating the bar plot with error bars\n",
    "plt.bar(months, mean_errors, yerr=error_bars, capsize=5)\n",
    "\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Error in MWH\")\n",
    "plt.title(\"Monthly Error with Upper Standard Deviation\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the dataframe for mean, lower, and upper bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_df.drop([\"Predicted Test Output\"], axis = 1, inplace = True)\n",
    "testing_df.insert(1, \"99% lower_bound\", test_uncertainty_df[\"99% lower_bound\"])\n",
    "testing_df.insert(2, \"99.99% lower_bound\", test_uncertainty_df[\"99.99% lower_bound\"])\n",
    "testing_df.insert(3, \"99.999% lower_bound\", test_uncertainty_df[\"99.999% lower_bound\"])\n",
    "testing_df.insert(4, \"mean\", test_uncertainty_df[\"energy_demand_mean\"])\n",
    "testing_df.insert(5, \"99.999% upper_bound\", test_uncertainty_df[\"99.999% upper_bound\"])\n",
    "testing_df.insert(6, \"99.99% upper_bound\", test_uncertainty_df[\"99.99% upper_bound\"])\n",
    "testing_df.insert(7, \"99% upper_bound\", test_uncertainty_df[\"99% upper_bound\"])\n",
    "testing_df.to_csv(\"TestingDataBounds2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df.drop([\"Predicted Val Output\"], axis = 1, inplace = True)\n",
    "validation_df.insert(1, \"99% lower_bound\", validation_uncertainty_df[\"99% lower_bound\"])\n",
    "validation_df.insert(2, \"99.99% lower_bound\", validation_uncertainty_df[\"99.99% lower_bound\"])\n",
    "validation_df.insert(3, \"99.999% lower_bound\", validation_uncertainty_df[\"99.999% lower_bound\"])\n",
    "validation_df.insert(4, \"mean\", validation_uncertainty_df[\"energy_demand_mean\"])\n",
    "validation_df.insert(5, \"99.999% upper_bound\", validation_uncertainty_df[\"99.999% upper_bound\"])\n",
    "validation_df.insert(6, \"99.99% upper_bound\", validation_uncertainty_df[\"99.99% upper_bound\"])\n",
    "validation_df.insert(7, \"99% upper_bound\", validation_uncertainty_df[\"99% upper_bound\"])\n",
    "validation_df.to_csv(\"ValidationDataBounds2.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.drop([\"Predicted Train Output\"], axis = 1, inplace = True)\n",
    "training_df.insert(1, \"99% lower_bound\", training_uncertainty_df[\"99% lower_bound\"])\n",
    "training_df.insert(2, \"99.99% lower_bound\", training_uncertainty_df[\"99.99% lower_bound\"])\n",
    "training_df.insert(3, \"99.999% lower_bound\", training_uncertainty_df[\"99.999% lower_bound\"])\n",
    "training_df.insert(4, \"mean\", training_uncertainty_df[\"energy_demand_mean\"])\n",
    "training_df.insert(5, \"99.999% upper_bound\", training_uncertainty_df[\"99.999% upper_bound\"])\n",
    "training_df.insert(6, \"99.99% upper_bound\", training_uncertainty_df[\"99.99% upper_bound\"])\n",
    "training_df.insert(7, \"99% upper_bound\", training_uncertainty_df[\"99% upper_bound\"])\n",
    "training_df.to_csv(\"TrainingDataBounds2.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLrYy9qgxp_C"
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "- Bayesian LSTMs have been able to produce comparable performance to their frequentist counterparts (all else being equal)\n",
    "- Stochastic dropout enables users to approximate the posterior distribution of the target variable, \\\n",
    "and thus construct confidence intervals for each prediction\n",
    "- Bayesian Neural Networks only attempt to account for epistemic model uncertainty and do not necessarily address aleatoric uncertainty\n",
    "- Computational overhead for repeated/multiple Bayesian LSTM predictions at inference to construct confidence intervals represent a potential challenge for real-time inference use-cases."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
